{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ee8dbe",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One cell: load 1 cache item, resolve codec/vocab/pad from cache+ckpt, plot, run, decode, audio + plots\n",
    "from pathlib import Path\n",
    "import json, numpy as np, torch\n",
    "\n",
    "CACHE_DIR = Path(\"cache/encodec_acoustic\")  # xcodec_acoustic, dac_acoustic\n",
    "CKPT_PATH = Path(\"artifacts/checkpoints/encodec_small_single_kit.pt\") # expressivegrid_to_xcodec, expressivegrid_to_dac\n",
    "DEVICE = \"cuda:0\"         # model forward\n",
    "DECODE_DEVICE = \"cuda:0\"  # codec decode (set to \"cpu\" if VRAM is tight)\n",
    "EVAL_SR = 32000\n",
    "\n",
    "# --- pick 1 item ---\n",
    "manifest = sorted(CACHE_DIR.glob(\"manifest_midigroove_test_*.jsonl\"))[0]\n",
    "rec = json.loads(manifest.read_text().splitlines()[0])\n",
    "npz_path = Path(rec[\"npz\"])\n",
    "print(\"NPZ:\", npz_path)\n",
    "\n",
    "with np.load(npz_path, allow_pickle=False) as d:\n",
    "    ex = {k: np.asarray(d[k]) for k in d.files}\n",
    "\n",
    "# --- load conditioning + targets ---\n",
    "drum_hit = ex[\"drum_hit\"].astype(np.float32)                                # [D,T]\n",
    "drum_vel = ex.get(\"drum_vel\", np.zeros_like(drum_hit)).astype(np.float32)   # [D,T]\n",
    "drum_sus = ex.get(\"drum_sustain\", np.zeros_like(drum_hit)).astype(np.float32)  # [D,T]\n",
    "hh_cc4   = ex.get(\"hh_open_cc4\", np.zeros((drum_hit.shape[1],), np.float32)).astype(np.float32)  # [T]\n",
    "beat_pos = ex[\"beat_pos\"].astype(np.int64)                                  # [T]\n",
    "bpm = float(ex.get(\"bpm\", 120.0))\n",
    "drummer_id = int(ex.get(\"drummer_id\", 0))\n",
    "tgt = ex[\"tgt\"].astype(np.int64)                                            # [C,T]\n",
    "D, T = drum_hit.shape\n",
    "C = int(tgt.shape[0])\n",
    "print(f\"D={D} T={T} C={C} bpm={bpm} drummer_id={drummer_id}\")\n",
    "\n",
    "# --- resolve codec from cache semantics (fallback to ckpt cfg) ---\n",
    "cache_sem = json.loads(str(ex.get(\"semantics\", np.asarray(\"{}\")).item() or \"{}\"))\n",
    "cache_codec = str(cache_sem.get(\"encoder\", \"\") or \"\").strip().lower() or None\n",
    "\n",
    "from midigroove_poc import expressivegrid as eg\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "cfg = ckpt.get(\"cfg\", {}) if isinstance(ckpt.get(\"cfg\", {}), dict) else {}\n",
    "ckpt_codec = str(cfg.get(\"encoder_model\", \"\") or \"\").strip().lower() or None\n",
    "codec = cache_codec or ckpt_codec or \"encodec\"\n",
    "\n",
    "# --- resolve vocab_size / pad_id from ckpt (new) or infer from head (old) ---\n",
    "num_codebooks = int(ckpt[\"num_codebooks\"])\n",
    "state = ckpt[\"model\"]\n",
    "vocab_size = int(cfg.get(\"vocab_size\", 0) or 0)\n",
    "if vocab_size <= 0:\n",
    "    vs = eg._infer_vocab_size_from_state_dict(state, num_codebooks=num_codebooks)\n",
    "    vocab_size = int(vs) if vs is not None else int(eg._vocab_size_for_codebook(eg._default_codebook_size_for_encoder(codec)))\n",
    "pad_id = int(cfg.get(\"pad_id\", vocab_size - 1 if vocab_size > 1 else 2048))\n",
    "\n",
    "print(\"resolved codec:\", codec, \"vocab_size:\", vocab_size, \"pad_id:\", pad_id)\n",
    "\n",
    "# --- plotting inputs (always show all lanes) ---\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install matplotlib in this env (e.g. `pip install matplotlib`)\") from e\n",
    "\n",
    "from data.midigroove_encodec_dataset import CHANNELS\n",
    "fig, axs = plt.subplots(5, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "axs[0].imshow(drum_hit, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
    "axs[0].set_title(\"drum_hit [D,T]\")\n",
    "axs[0].set_yticks(range(len(CHANNELS)))\n",
    "axs[0].set_yticklabels(CHANNELS, fontsize=8)\n",
    "\n",
    "axs[1].imshow(drum_vel, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
    "axs[1].set_title(\"drum_vel [D,T]\")\n",
    "\n",
    "axs[2].imshow(drum_sus, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
    "axs[2].set_title(\"drum_sustain [D,T] (optional)\")\n",
    "\n",
    "axs[3].plot(hh_cc4)\n",
    "axs[3].set_ylim(-0.05, 1.05)\n",
    "axs[3].set_title(\"hh_open_cc4 [T] (optional)\")\n",
    "\n",
    "axs[4].imshow(beat_pos[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\", vmin=0, vmax=3)\n",
    "axs[4].set_title(\"beat_pos [T] (0..3)\")\n",
    "axs[4].set_xlabel(\"frame\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- build input grid respecting ckpt feature flags ---\n",
    "include_sustain = bool(cfg.get(\"include_sustain\", False))\n",
    "include_hh_cc4 = bool(cfg.get(\"include_hh_cc4\", False))\n",
    "\n",
    "pieces = [drum_hit, drum_vel]\n",
    "if include_sustain:\n",
    "    pieces.append(drum_sus)\n",
    "if include_hh_cc4:\n",
    "    pieces.append(hh_cc4[None, :])\n",
    "\n",
    "grid = np.concatenate(pieces, axis=0).astype(np.float32)  # [F,T]\n",
    "in_dim = int(ckpt[\"in_dim\"])\n",
    "assert grid.shape[0] == in_dim and grid.shape[1] == T, (grid.shape, in_dim, T)\n",
    "\n",
    "# --- run checkpoint -> predicted tokens ---\n",
    "cfg2 = dict(cfg)\n",
    "cfg2.setdefault(\"encoder_model\", codec)\n",
    "cfg2[\"vocab_size\"] = int(vocab_size)\n",
    "cfg2[\"pad_id\"] = int(pad_id)\n",
    "\n",
    "model = eg._build_model(num_codebooks=num_codebooks, in_dim=in_dim, cfg=cfg2)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.to(torch.device(DEVICE)).eval()\n",
    "\n",
    "grid_t = torch.from_numpy(grid).unsqueeze(0).to(DEVICE)            # [1,F,T]\n",
    "beat_pos_t = torch.from_numpy(beat_pos).unsqueeze(0).to(DEVICE)    # [1,T]\n",
    "bpm_t = torch.tensor([bpm], dtype=torch.float32, device=DEVICE)    # [1]\n",
    "drummer_id_t = torch.tensor([drummer_id], dtype=torch.long, device=DEVICE)  # [1]\n",
    "valid_mask_t = torch.ones((1, T), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    logits = model(grid=grid_t, beat_pos=beat_pos_t, bpm=bpm_t, drummer_id=drummer_id_t, valid_mask=valid_mask_t)  # [1,C,T,V]\n",
    "    pred = logits.argmax(dim=-1).squeeze(0).to(torch.long).cpu()  # [C,T]\n",
    "\n",
    "tgt_t = torch.from_numpy(tgt).to(torch.long)  # [C,T]\n",
    "mask = tgt_t.ne(pad_id)\n",
    "tok_acc_masked = float(((pred == tgt_t) & mask).sum().item() / max(1, mask.sum().item()))\n",
    "print(\"used sustain:\", include_sustain, \"used hh_cc4:\", include_hh_cc4)\n",
    "print(\"token_acc(masked):\", tok_acc_masked, \"pred shape:\", tuple(pred.shape), \"tgt shape:\", tuple(tgt_t.shape))\n",
    "\n",
    "# for decoding: map PAD -> 0\n",
    "pred_clean = torch.where(pred == pad_id, torch.zeros_like(pred), pred)\n",
    "tgt_clean = torch.where(tgt_t == pad_id, torch.zeros_like(tgt_t), tgt_t)\n",
    "\n",
    "# --- decode audio (prediction vs gt tokens) and compare to original segment ---\n",
    "from IPython.display import Audio, display\n",
    "from data.codecs import decode_tokens_to_audio\n",
    "from midigroove_poc.eval import _load_audio_segment, _resample_linear\n",
    "\n",
    "audio_path = Path(str(ex[\"audio_path\"].item()))\n",
    "sr_native = int(ex[\"sr\"].item())\n",
    "start_sec = float(ex[\"start_sec\"].item())\n",
    "window_seconds = float(ex[\"window_seconds\"].item())\n",
    "start_sample = int(round(start_sec * sr_native))\n",
    "window_samples = int(round(window_seconds * sr_native))\n",
    "\n",
    "ref, sr_ref = _load_audio_segment(audio_path, start_sample=start_sample, num_samples=window_samples)\n",
    "ref_rs = _resample_linear(ref, sr_ref, EVAL_SR)\n",
    "\n",
    "audio_gt_b1, sr_gt = decode_tokens_to_audio(tgt_clean, encoder_model=codec, device=DECODE_DEVICE)\n",
    "audio_pr_b1, sr_pr = decode_tokens_to_audio(pred_clean, encoder_model=codec, device=DECODE_DEVICE)\n",
    "gt_rs = _resample_linear(audio_gt_b1[0], sr_gt, EVAL_SR)\n",
    "pr_rs = _resample_linear(audio_pr_b1[0], sr_pr, EVAL_SR)\n",
    "\n",
    "N = min(ref_rs.size, gt_rs.size, pr_rs.size)\n",
    "ref_rs, gt_rs, pr_rs = ref_rs[:N], gt_rs[:N], pr_rs[:N]\n",
    "\n",
    "print(\"ref_rs:\", ref_rs.shape, \"gt_rs:\", gt_rs.shape, \"pr_rs:\", pr_rs.shape)\n",
    "\n",
    "print(\"Original audio (resampled):\")\n",
    "display(Audio(ref_rs, rate=EVAL_SR))\n",
    "print(f\"Codec reconstruction from ground-truth tokens ({codec}) (resampled):\")\n",
    "display(Audio(gt_rs, rate=EVAL_SR))\n",
    "print(f\"Model prediction decoded ({codec}) (resampled):\")\n",
    "display(Audio(pr_rs, rate=EVAL_SR))\n",
    "\n",
    "# --- waveform comparison plots ---\n",
    "import numpy as np\n",
    "def l1(a, b): return float(np.mean(np.abs(a - b)))\n",
    "print(\"L1(gt vs ref):\", l1(gt_rs, ref_rs))\n",
    "print(\"L1(pred vs ref):\", l1(pr_rs, ref_rs))\n",
    "\n",
    "t = np.arange(N) / float(EVAL_SR)\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(t, ref_rs, label=\"ref (resampled)\", alpha=0.7)\n",
    "plt.plot(t, gt_rs, label=f\"gt decode ({codec})\", alpha=0.7)\n",
    "plt.plot(t, pr_rs, label=f\"pred decode ({codec})\", alpha=0.7)\n",
    "plt.xlim(0, min(t[-1], 2.0))\n",
    "plt.legend()\n",
    "plt.title(\"Waveforms (first ~2s)\")\n",
    "plt.xlabel(\"seconds\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ee1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plotting: main (waveform + hit + vel) and optionals (sus + cc4 + beat_pos) ---\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install matplotlib in this env (e.g. `pip install matplotlib`)\") from e\n",
    "\n",
    "from data.midigroove_encodec_dataset import CHANNELS\n",
    "from midigroove_poc.eval import _load_audio_segment, _resample_linear\n",
    "\n",
    "audio_path = Path(str(ex[\"audio_path\"].item()))\n",
    "sr_native = int(ex[\"sr\"].item())\n",
    "start_sec = float(ex[\"start_sec\"].item())\n",
    "window_seconds = float(ex[\"window_seconds\"].item())\n",
    "start_sample = int(round(start_sec * sr_native))\n",
    "window_samples = int(round(window_seconds * sr_native))\n",
    "\n",
    "ref, sr_ref = _load_audio_segment(audio_path, start_sample=start_sample, num_samples=window_samples)\n",
    "ref_rs = _resample_linear(ref, sr_ref, EVAL_SR)\n",
    "\n",
    "t_sec = np.arange(ref_rs.size) / float(EVAL_SR)\n",
    "extent_dt = [0.0, float(window_seconds), 0.0, float(D)]  # x=seconds, y=drum lanes\n",
    "\n",
    "# Main figure: waveform + drum_hit + drum_vel\n",
    "fig1, axs1 = plt.subplots(4, 1, figsize=(14, 9), sharex=True, gridspec_kw={\"height_ratios\": [1, 1.6, 1.6, 0.7]})\n",
    "\n",
    "axs1[0].plot(t_sec, ref_rs, lw=0.8)\n",
    "axs1[0].set_title(\"waveform (original segment, resampled)\")\n",
    "axs1[0].set_ylabel(\"amp\")\n",
    "\n",
    "axs1[1].imshow(drum_hit, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\", extent=extent_dt)\n",
    "axs1[1].set_title(\"drum_hit [D,T]\")\n",
    "axs1[1].set_yticks(np.arange(D) + 0.5)\n",
    "axs1[1].set_yticklabels(CHANNELS, fontsize=8)\n",
    "\n",
    "axs1[2].imshow(drum_vel, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\", extent=extent_dt)\n",
    "axs1[2].set_title(\"drum_vel [D,T]\")\n",
    "axs1[2].set_yticks(np.arange(D) + 0.5)\n",
    "axs1[2].set_yticklabels(CHANNELS, fontsize=8)\n",
    "axs1[2].set_xlabel(\"seconds\")\n",
    "\n",
    "axs1[3].imshow(beat_pos[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
    "               extent=[0.0, float(window_seconds), 0.0, 1.0], vmin=0, vmax=3)\n",
    "axs1[3].set_title(\"beat_pos [T] (0..3)\")\n",
    "axs1[3].set_yticks([])\n",
    "axs1[3].set_xlabel(\"seconds\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional figure: drum_sustain + hh_open_cc4 + beat_pos\n",
    "fig2, axs2 = plt.subplots(3, 1, figsize=(14, 7), sharex=True, gridspec_kw={\"height_ratios\": [1.6, 1.0, 0.7]})\n",
    "\n",
    "axs2[0].imshow(drum_sus, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\", extent=extent_dt)\n",
    "axs2[0].set_title(\"drum_sustain [D,T] (optional)\")\n",
    "axs2[0].set_yticks(np.arange(D) + 0.5)\n",
    "axs2[0].set_yticklabels(CHANNELS, fontsize=8)\n",
    "\n",
    "axs2[1].plot(np.linspace(0.0, float(window_seconds), num=T, endpoint=False), hh_cc4, lw=0.9)\n",
    "axs2[1].set_ylim(-0.05, 1.05)\n",
    "axs2[1].set_title(\"hh_open_cc4 [T] (optional)\")\n",
    "axs2[1].set_ylabel(\"cc4\")\n",
    "\n",
    "axs2[2].imshow(beat_pos[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
    "               extent=[0.0, float(window_seconds), 0.0, 1.0], vmin=0, vmax=3)\n",
    "axs2[2].set_title(\"beat_pos [T] (0..3)\")\n",
    "axs2[2].set_yticks([])\n",
    "axs2[2].set_xlabel(\"seconds\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kit_name'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741cafeb",
   "metadata": {},
   "source": [
    "# LAtex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39915cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```latex\n",
       "\\begin{table*}[htbp]\n",
       "\\caption{Evaluation on one-kit and all-kits test sets (small models).}\n",
       "\\begin{center}\n",
       "\\renewcommand{\\arraystretch}{1}\n",
       "\\setlength{\\tabcolsep}{1pt}\n",
       "\\resizebox{\\textwidth}{!}{%\n",
       "\\begin{tabular}{|p{0.060\\linewidth}|p{0.060\\linewidth}|p{0.060\\linewidth}|p{0.060\\linewidth}|p{0.060\\linewidth}|p{0.07\\linewidth}|p{0.07\\linewidth}|p{0.11\\linewidth}|p{0.11\\linewidth}|p{0.12\\linewidth}|p{0.067\\linewidth}|p{0.067\\linewidth}|p{0.067\\linewidth}|p{0.064\\linewidth}|}\n",
       "\\hline\n",
       "\\textbf{Eval} & \\textbf{Codec} & \\multicolumn{3}{c|}{\\textbf{Token metrics}} & \\multicolumn{5}{c|}{\\textbf{Audio metrics}} & \\multicolumn{3}{c|}{\\textbf{Onset metrics}} & \\\\\n",
       "\\cline{3-5}\\cline{6-10}\\cline{11-13}\n",
       "\\textbf{Setting} & \\textbf{} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{NLL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{PPL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{Acc(\\%)}$^{\\mathrm{a}}$\\,\\,$\\uparrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{RMSE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{MAE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{MR-STFT SC}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{Env RMS corr}$^{\\mathrm{b}}$\\,\\,$\\uparrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{TTER (dB) MAE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{P(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{R(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{F1(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$}} & \\shortstack[c]{\\scriptsize \\textbf{\\textit{FAD}$^{\\mathrm{d}}$\\,\\,$\\downarrow$}} \\\\\n",
       "\\hline\n",
       " OneKit & encodec & \\cellcolor{green!15} $2.142\\pm0.681$ & \\cellcolor{green!15} $10.8\\pm8.3$ & \\cellcolor{green!15} $42.7\\pm13.7$ & $0.0201\\pm0.0106$ & $0.0100\\pm0.0058$ & \\cellcolor{green!15} $0.842\\pm0.160$ & \\cellcolor{green!15} $0.690\\pm0.228$ & \\cellcolor{green!15} $1.29\\pm1.22$ & \\cellcolor{green!15} $78.3\\pm15.5$ & \\cellcolor{green!15} $68.4\\pm17.8$ & \\cellcolor{green!15} $71.0\\pm13.3$ & \\cellcolor{green!15} $0.281$ \\\\\n",
       "\\hline\n",
       " OneKit & xcodec & $4.422\\pm0.590$ & $102.1\\pm81.8$ & $11.9\\pm3.9$ & $0.0305\\pm0.0176$ & $0.0161\\pm0.0110$ & $1.357\\pm0.702$ & $0.552\\pm0.242$ & $1.92\\pm1.60$ & $76.6\\pm17.6$ & $64.8\\pm16.5$ & $67.8\\pm12.4$ & $0.350$ \\\\\n",
       "\\hline\n",
       " OneKit & dac & $6.265\\pm0.450$ & $563.5\\pm236.7$ & $3.8\\pm6.0$ & \\cellcolor{green!15} $0.0184\\pm0.0094$ & \\cellcolor{green!15} $0.0095\\pm0.0054$ & $0.982\\pm0.085$ & $0.580\\pm0.235$ & $1.44\\pm1.24$ & $69.8\\pm15.1$ & $65.0\\pm18.1$ & $65.0\\pm12.3$ & $0.545$ \\\\\n",
       "\\hline\n",
       "\\hline\n",
       " AllKits & encodec & \\cellcolor{green!15} $2.153\\pm0.743$ & \\cellcolor{green!15} $11.6\\pm11.0$ & \\cellcolor{green!15} $43.4\\pm14.7$ & $0.0200\\pm0.0118$ & $0.0103\\pm0.0070$ & \\cellcolor{green!15} $0.827\\pm0.171$ & \\cellcolor{green!15} $0.710\\pm0.220$ & \\cellcolor{green!15} $1.47\\pm1.30$ & $78.2\\pm15.8$ & \\cellcolor{green!15} $68.2\\pm18.2$ & \\cellcolor{green!15} $70.6\\pm13.4$ & \\cellcolor{green!15} $0.193$ \\\\\n",
       "\\hline\n",
       " AllKits & xcodec & $4.429\\pm0.617$ & $104.9\\pm90.0$ & $12.5\\pm4.3$ & $0.0336\\pm0.0203$ & $0.0176\\pm0.0125$ & $1.669\\pm0.925$ & $0.568\\pm0.252$ & $2.22\\pm1.89$ & \\cellcolor{green!15} $78.9\\pm16.5$ & $64.4\\pm16.7$ & $68.8\\pm12.7$ & $0.277$ \\\\\n",
       "\\hline\n",
       " AllKits & dac & $6.153\\pm0.540$ & $521.8\\pm304.5$ & $4.7\\pm6.8$ & \\cellcolor{green!15} $0.0190\\pm0.0109$ & \\cellcolor{green!15} $0.0099\\pm0.0065$ & $1.034\\pm0.179$ & $0.602\\pm0.265$ & $1.99\\pm1.66$ & $75.7\\pm15.6$ & $68.1\\pm17.5$ & $69.3\\pm12.2$ & $0.405$ \\\\\n",
       "\\hline\n",
       "\\end{tabular}%\n",
       "}\n",
       "\\vspace{2pt}\n",
       "\\parbox{\\linewidth}{\\footnotesize $^{\\mathrm{a}}$PAD ignored; mean$\\pm$std over windows. $^{\\mathrm{b}}$Tokens decoded then resampled to 32\\,kHz; mean$\\pm$std over windows. $^{\\mathrm{c}}$Onset metrics match predicted-audio onsets to grid-derived GT onsets within 50\\,ms (GT velocity$\\ge$0.30). $^{\\mathrm{d}}$fadtk embedding: CLAP-LAION-music; per-run variant: OneKit: FAD$\\infty$, clap-laion-music, n=1748, clip~med=2.286s; AllKits: FAD$\\infty$, clap-laion-music, n=68180, clip~med=2.286s.}\n",
       "\\label{tab:eval_small_models}\n",
       "\\end{center}\n",
       "\\end{table*}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Big/Small models (one-kit + all-kits) → LaTeX table\n",
    "# Applies your exact p{..} widths, fixes FAD header (single occurrence), adds OneKit/AllKits separator.\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "MODEL_SIZE = \"small\"  # \"small\" | \"big\"\n",
    "\n",
    "RUNS = [\n",
    "    (\"OneKit\", Path(f\"artifacts/eval/{MODEL_SIZE}_one_kit/summary.json\")),\n",
    "    (\"AllKits\", Path(f\"artifacts/eval/{MODEL_SIZE}_all_kits/summary.json\")),\n",
    "]\n",
    "SYSTEMS = [\"encodec\", \"xcodec\", \"dac\"]\n",
    "\n",
    "TABLE_ENV = \"table*\"\n",
    "DASH = r\"---\"\n",
    "HICOLOR = r\"green!15\"\n",
    "\n",
    "# Exact column widths you provided\n",
    "COLSPEC = (\n",
    "    r\"|p{0.060\\linewidth}|p{0.060\\linewidth}|\"\n",
    "    r\"p{0.060\\linewidth}|p{0.060\\linewidth}|p{0.060\\linewidth}|\"\n",
    "    r\"p{0.07\\linewidth}|p{0.07\\linewidth}|p{0.11\\linewidth}|p{0.11\\linewidth}|p{0.12\\linewidth}|\"\n",
    "    r\"p{0.067\\linewidth}|p{0.067\\linewidth}|p{0.067\\linewidth}|\"\n",
    "    r\"p{0.064\\linewidth}|\"\n",
    ")\n",
    "\n",
    "def _get_metric_dict(s, keys):\n",
    "    for k in keys:\n",
    "        if k in s:\n",
    "            return s[k]\n",
    "    return {\"mean\": np.nan, \"std\": np.nan}\n",
    "\n",
    "def _mean(x):\n",
    "    return float(x.get(\"mean\", np.nan))\n",
    "\n",
    "def pm(x, digits=3):\n",
    "    m = _mean(x); sd = float(x.get(\"std\", np.nan))\n",
    "    return DASH if (not np.isfinite(m) or not np.isfinite(sd)) else f\"${m:.{digits}f}\\\\pm{sd:.{digits}f}$\"\n",
    "\n",
    "def pm_pct(x, digits=1):\n",
    "    m = _mean(x); sd = float(x.get(\"std\", np.nan))\n",
    "    return DASH if (not np.isfinite(m) or not np.isfinite(sd)) else f\"${(100*m):.{digits}f}\\\\pm{(100*sd):.{digits}f}$\"\n",
    "\n",
    "def fnum(v, digits=3):\n",
    "    try:\n",
    "        v = float(v)\n",
    "    except Exception:\n",
    "        return DASH\n",
    "    return DASH if not np.isfinite(v) else f\"${v:.{digits}f}$\"\n",
    "\n",
    "def fadtk_key(sys_dict):\n",
    "    for k in sys_dict.keys():\n",
    "        if str(k).startswith(\"fad_fadtk_\"):\n",
    "            return str(k)\n",
    "    return None\n",
    "\n",
    "def fad_val(run_summary, sys_name):\n",
    "    s = run_summary[\"systems\"].get(sys_name, {})\n",
    "    k = fadtk_key(s)\n",
    "    return np.nan if not k else float((s.get(k, {}) or {}).get(\"fad\", np.nan))\n",
    "\n",
    "def fadinf_val(run_summary, sys_name):\n",
    "    s = run_summary[\"systems\"].get(sys_name, {})\n",
    "    k = fadtk_key(s)\n",
    "    if not k:\n",
    "        return np.nan\n",
    "    fi = (s.get(k, {}) or {}).get(\"fad_inf\", {}) or {}\n",
    "    return float(fi.get(\"fad_inf\", np.nan))\n",
    "\n",
    "METRICS = [\n",
    "    dict(name=\"token_nll\",    keys=[\"token_nll\"],    fmt=pm,     digits=3, dir=\"min\"),\n",
    "    dict(name=\"token_ppl\",    keys=[\"token_ppl\"],    fmt=pm,     digits=1, dir=\"min\"),\n",
    "    dict(name=\"token_acc\",    keys=[\"token_acc\"],    fmt=pm_pct, digits=1, dir=\"max\"),\n",
    "\n",
    "    dict(name=\"rmse\",         keys=[\"rmse\"],         fmt=pm,     digits=4, dir=\"min\"),\n",
    "    dict(name=\"mae\",          keys=[\"mae\"],          fmt=pm,     digits=4, dir=\"min\"),\n",
    "    dict(name=\"mr_stft_sc\",   keys=[\"mr_stft_sc\"],   fmt=pm,     digits=3, dir=\"min\"),\n",
    "    dict(name=\"env_rms_corr\", keys=[\"env_rms_corr\"], fmt=pm,     digits=3, dir=\"max\"),\n",
    "    dict(name=\"tter_db_mae\",  keys=[\"tter_db_mae\"],  fmt=pm,     digits=2, dir=\"min\"),\n",
    "\n",
    "    dict(name=\"onset_precision\", keys=[\"onset_precision\",\"onset_prec\",\"onset_p\",\"onset_pr\"], fmt=pm_pct, digits=1, dir=\"max\"),\n",
    "    dict(name=\"onset_recall\",    keys=[\"onset_recall\",\"onset_rec\",\"onset_r\"],                fmt=pm_pct, digits=1, dir=\"max\"),\n",
    "    dict(name=\"onset_f1\",        keys=[\"onset_f1\"],                                        fmt=pm_pct, digits=1, dir=\"max\"),\n",
    "]\n",
    "\n",
    "# ---- load summaries ----\n",
    "summ = {run_label: json.loads(p.read_text()) for run_label, p in RUNS}\n",
    "\n",
    "# ---- choose per-run FAD column value: FAD∞ if present, else FAD ----\n",
    "run_fad_kind = {}\n",
    "for run_label, _ in RUNS:\n",
    "    has_inf = any(np.isfinite(fadinf_val(summ[run_label], s)) for s in SYSTEMS)\n",
    "    run_fad_kind[run_label] = \"inf\" if has_inf else \"fad\"\n",
    "\n",
    "def fad_value(run_label, sys_name):\n",
    "    return fadinf_val(summ[run_label], sys_name) if run_fad_kind[run_label] == \"inf\" else fad_val(summ[run_label], sys_name)\n",
    "\n",
    "# ---- best cells ----\n",
    "best_cells = set()  # (run_label, sys_name, metric_name)\n",
    "\n",
    "for run_label, _ in RUNS:\n",
    "    sysdict = summ[run_label][\"systems\"]\n",
    "    for ms in METRICS:\n",
    "        vals = []\n",
    "        for sys_name in SYSTEMS:\n",
    "            x = _get_metric_dict(sysdict[sys_name], ms[\"keys\"])\n",
    "            m = _mean(x)\n",
    "            if np.isfinite(m):\n",
    "                vals.append((sys_name, m))\n",
    "        if vals:\n",
    "            best_sys = min(vals, key=lambda t: t[1])[0] if ms[\"dir\"] == \"min\" else max(vals, key=lambda t: t[1])[0]\n",
    "            best_cells.add((run_label, best_sys, ms[\"name\"]))\n",
    "\n",
    "for run_label, _ in RUNS:\n",
    "    vals = [(s, fad_value(run_label, s)) for s in SYSTEMS]\n",
    "    vals = [(s, v) for (s, v) in vals if np.isfinite(v)]\n",
    "    if vals:\n",
    "        best_cells.add((run_label, min(vals, key=lambda t: t[1])[0], \"fad_col\"))\n",
    "\n",
    "def format_metric(run_label, sys_name, sysdict, metric_name):\n",
    "    ms = next(m for m in METRICS if m[\"name\"] == metric_name)\n",
    "    x = _get_metric_dict(sysdict, ms[\"keys\"])\n",
    "    out = ms[\"fmt\"](x, ms[\"digits\"])\n",
    "    if out != DASH and (run_label, sys_name, metric_name) in best_cells:\n",
    "        return rf\"\\cellcolor{{{HICOLOR}}} {out}\"\n",
    "    return out\n",
    "\n",
    "def format_fad(run_label, sys_name):\n",
    "    out = fnum(fad_value(run_label, sys_name), digits=3)\n",
    "    if out != DASH and (run_label, sys_name, \"fad_col\") in best_cells:\n",
    "        return rf\"\\cellcolor{{{HICOLOR}}} {out}\"\n",
    "    return out\n",
    "\n",
    "def row_for(run_label, sys_name):\n",
    "    s = summ[run_label][\"systems\"][sys_name]\n",
    "    return [\n",
    "        run_label,\n",
    "        sys_name,\n",
    "        format_metric(run_label, sys_name, s, \"token_nll\"),\n",
    "        format_metric(run_label, sys_name, s, \"token_ppl\"),\n",
    "        format_metric(run_label, sys_name, s, \"token_acc\"),\n",
    "        format_metric(run_label, sys_name, s, \"rmse\"),\n",
    "        format_metric(run_label, sys_name, s, \"mae\"),\n",
    "        format_metric(run_label, sys_name, s, \"mr_stft_sc\"),\n",
    "        format_metric(run_label, sys_name, s, \"env_rms_corr\"),\n",
    "        format_metric(run_label, sys_name, s, \"tter_db_mae\"),\n",
    "        format_metric(run_label, sys_name, s, \"onset_precision\"),\n",
    "        format_metric(run_label, sys_name, s, \"onset_recall\"),\n",
    "        format_metric(run_label, sys_name, s, \"onset_f1\"),\n",
    "        format_fad(run_label, sys_name),\n",
    "    ]\n",
    "\n",
    "def fadtk_note(run_label):\n",
    "    m = summ[run_label].get(\"fadtk\", {}) or {}\n",
    "    model = m.get(\"model\", \"clap-laion-music\")\n",
    "    n_used = (m.get(\"reference\", {}) or {}).get(\"n_items_fad\", m.get(\"n_used\", \"\"))\n",
    "    clip = (m.get(\"clip_dur_s\", {}) or {})\n",
    "    clip_med = clip.get(\"median\", np.nan)\n",
    "    clip_med_s = f\"{clip_med:.3f}s\" if np.isfinite(clip_med) else \"?\"\n",
    "    which = \"FAD$\\\\infty$\" if run_fad_kind[run_label] == \"inf\" else \"FAD\"\n",
    "    return f\"{run_label}: {which}, {model}, n={n_used}, clip~med={clip_med_s}\"\n",
    "\n",
    "def H(tex):\n",
    "    return rf\"\\shortstack[c]{{\\scriptsize {tex}}}\"\n",
    "\n",
    "caption = f\"Evaluation on one-kit and all-kits test sets ({MODEL_SIZE} models).\"\n",
    "label = f\"tab:eval_{MODEL_SIZE}_models\"\n",
    "\n",
    "# ---- build LaTeX ----\n",
    "lines = []\n",
    "lines.append(rf\"\\begin{{{TABLE_ENV}}}[htbp]\")\n",
    "lines.append(rf\"\\caption{{{caption}}}\")\n",
    "lines.append(r\"\\begin{center}\")\n",
    "lines.append(r\"\\renewcommand{\\arraystretch}{1}\")\n",
    "lines.append(r\"\\setlength{\\tabcolsep}{1pt}\")\n",
    "lines.append(r\"\\resizebox{\\textwidth}{!}{%\")\n",
    "lines.append(r\"\\begin{tabular}{\" + COLSPEC + r\"}\")\n",
    "lines.append(r\"\\hline\")\n",
    "\n",
    "# Header row 1: group headers; keep FAD cell empty to avoid repeating “FAD”\n",
    "lines.append(\n",
    "    r\"\\textbf{Eval} & \\textbf{Codec} \"\n",
    "    r\"& \\multicolumn{3}{c|}{\\textbf{Token metrics}} \"\n",
    "    r\"& \\multicolumn{5}{c|}{\\textbf{Audio metrics}} \"\n",
    "    r\"& \\multicolumn{3}{c|}{\\textbf{Onset metrics}} \"\n",
    "    r\"& \\\\\"\n",
    ")\n",
    "lines.append(r\"\\cline{3-5}\\cline{6-10}\\cline{11-13}\")\n",
    "\n",
    "# Header row 2: per-metric titles (arrows inline) including a single FAD entry\n",
    "lines.append(\n",
    "    r\"\\textbf{Setting} & \\textbf{}\"\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{NLL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{PPL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{Acc(\\%)}$^{\\mathrm{a}}$\\,\\,$\\uparrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{RMSE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{MAE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{MR-STFT SC}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{Env RMS corr}$^{\\mathrm{b}}$\\,\\,$\\uparrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{TTER (dB) MAE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{P(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{R(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{F1(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$}\") +\n",
    "    r\" & \" + H(r\"\\textbf{\\textit{FAD}$^{\\mathrm{d}}$\\,\\,$\\downarrow$}\") +\n",
    "    r\" \\\\\"\n",
    ")\n",
    "lines.append(r\"\\hline\")\n",
    "\n",
    "# Body + separator between OneKit and AllKits\n",
    "for run_idx, (run_label, _p) in enumerate(RUNS):\n",
    "    for sys_name in SYSTEMS:\n",
    "        lines.append(\" \" + \" & \".join(row_for(run_label, sys_name)) + r\" \\\\\")\n",
    "        lines.append(r\"\\hline\")\n",
    "    if run_idx == 0 and len(RUNS) > 1:\n",
    "        lines.append(r\"\\hline\")  # extra separator line between blocks\n",
    "\n",
    "lines.append(r\"\\end{tabular}%\")\n",
    "lines.append(r\"}\")  # resizebox\n",
    "lines.append(r\"\\vspace{2pt}\")\n",
    "lines.append(\n",
    "    r\"\\parbox{\\linewidth}{\\footnotesize \"\n",
    "    r\"$^{\\mathrm{a}}$PAD ignored; mean$\\pm$std over windows. \"\n",
    "    r\"$^{\\mathrm{b}}$Tokens decoded then resampled to 32\\,kHz; mean$\\pm$std over windows. \"\n",
    "    r\"$^{\\mathrm{c}}$Onset metrics match predicted-audio onsets to grid-derived GT onsets within 50\\,ms (GT velocity$\\ge$0.30). \"\n",
    "    r\"$^{\\mathrm{d}}$fadtk embedding: CLAP-LAION-music; per-run variant: \"\n",
    "    + \"; \".join([fadtk_note(rl) for rl, _ in RUNS])\n",
    "    + r\".}\"\n",
    ")\n",
    "lines.append(rf\"\\label{{{label}}}\")\n",
    "lines.append(r\"\\end{center}\")\n",
    "lines.append(rf\"\\end{{{TABLE_ENV}}}\")\n",
    "\n",
    "latex = \"\\n\".join(lines)\n",
    "display(Markdown(\"```latex\\n\" + latex + \"\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ce4cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```latex\n",
       "\\begin{table*}[htbp]\n",
       "\\caption{Top-3 and worst-3 kits by EnCodec token accuracy (all-kits). Best values per kit highlighted.}\n",
       "\\begin{center}\n",
       "\\renewcommand{\\arraystretch}{0.95}\n",
       "\\setlength{\\tabcolsep}{0.6pt}\n",
       "\\scriptsize\n",
       "\\resizebox{\\textwidth}{!}{%\n",
       "\\begin{tabular}{|p{0.055\\linewidth}|p{0.165\\linewidth}|p{0.055\\linewidth}|p{0.050\\linewidth}|p{0.050\\linewidth}|p{0.055\\linewidth}|p{0.060\\linewidth}|p{0.060\\linewidth}|p{0.070\\linewidth}|p{0.070\\linewidth}|p{0.070\\linewidth}|p{0.052\\linewidth}|p{0.052\\linewidth}|p{0.052\\linewidth}|p{0.055\\linewidth}|}\n",
       "\\hline\n",
       "\\textbf{Eval} & \\textbf{Kit} & \\textbf{Codec} & \\multicolumn{3}{c|}{\\textbf{Token}} & \\multicolumn{5}{c|}{\\textbf{Audio}} & \\multicolumn{3}{c|}{\\textbf{Onset}} & \\\\\n",
       "\\cline{4-6}\\cline{7-11}\\cline{12-14}\n",
       "\\textbf{Set} & \\textbf{} & \\textbf{} & \\shortstack[c]{\\scriptsize \\textbf{NLL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$} & \\shortstack[c]{\\scriptsize \\textbf{PPL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$} & \\shortstack[c]{\\scriptsize \\textbf{Acc(\\%)}$^{\\mathrm{a}}$\\,\\,$\\uparrow$} & \\shortstack[c]{\\scriptsize \\textbf{RMSE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$} & \\shortstack[c]{\\scriptsize \\textbf{MAE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$} & \\shortstack[c]{\\scriptsize \\textbf{MRSTFT}$^{\\mathrm{b}}$\\,\\,$\\downarrow$} & \\shortstack[c]{\\scriptsize \\textbf{Env}$^{\\mathrm{b}}$\\,\\,$\\uparrow$} & \\shortstack[c]{\\scriptsize \\textbf{TTER}$^{\\mathrm{b}}$\\,\\,$\\downarrow$} & \\shortstack[c]{\\scriptsize \\textbf{P(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$} & \\shortstack[c]{\\scriptsize \\textbf{R(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$} & \\shortstack[c]{\\scriptsize \\textbf{F1(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$} & \\shortstack[c]{\\scriptsize \\textbf{FAD}$^{\\mathrm{d}}$\\,\\,$\\downarrow$} \\\\\n",
       "\\hline\n",
       " \\textbf{Top} & Shuffle (Blues) & dac & $6.17$ & $521.7$ & $4.8$ & \\cellcolor{green!15} $0.0131$ & \\cellcolor{green!15} $0.0063$ & $1.080$ & $0.68$ & $2.46$ & $78$ & \\cellcolor{green!15} $67$ & \\cellcolor{green!15} $70$ & $0.449$ \\\\\n",
       "  &  & encodec & \\cellcolor{green!15} $1.83$ & \\cellcolor{green!15} $7.9$ & \\cellcolor{green!15} $50.5$ & $0.0134$ & $0.0066$ & \\cellcolor{green!15} $0.826$ & \\cellcolor{green!15} $0.74$ & \\cellcolor{green!15} $1.40$ & \\cellcolor{green!15} $83$ & $58$ & $66$ & \\cellcolor{green!15} $0.332$ \\\\\n",
       "  &  & xcodec & $4.49$ & $109.8$ & $12.1$ & $0.0252$ & $0.0139$ & $2.072$ & $0.47$ & $2.10$ & $76$ & $66$ & $68$ & $0.407$ \\\\\n",
       "\\hline\n",
       " \\textbf{Top} & Warmer Funk & dac & $5.89$ & $429.1$ & $7.6$ & \\cellcolor{green!15} $0.0150$ & \\cellcolor{green!15} $0.0061$ & $0.996$ & $0.65$ & $1.86$ & $75$ & $70$ & $71$ & $0.408$ \\\\\n",
       "  &  & encodec & \\cellcolor{green!15} $1.89$ & \\cellcolor{green!15} $9.0$ & \\cellcolor{green!15} $50.2$ & $0.0164$ & $0.0068$ & \\cellcolor{green!15} $0.847$ & \\cellcolor{green!15} $0.70$ & \\cellcolor{green!15} $1.85$ & $77$ & \\cellcolor{green!15} $73$ & \\cellcolor{green!15} $73$ & \\cellcolor{green!15} $0.253$ \\\\\n",
       "  &  & xcodec & $4.44$ & $110.4$ & $12.2$ & $0.0267$ & $0.0107$ & $1.566$ & $0.64$ & $1.96$ & \\cellcolor{green!15} $79$ & $62$ & $68$ & $0.310$ \\\\\n",
       "\\hline\n",
       " \\textbf{Top} & 60s Rock & dac & $5.95$ & $444.1$ & $7.4$ & \\cellcolor{green!15} $0.0139$ & \\cellcolor{green!15} $0.0065$ & $0.973$ & $0.65$ & $1.83$ & $77$ & $67$ & $69$ & $0.346$ \\\\\n",
       "  &  & encodec & \\cellcolor{green!15} $1.84$ & \\cellcolor{green!15} $8.1$ & \\cellcolor{green!15} $49.9$ & $0.0153$ & $0.0071$ & \\cellcolor{green!15} $0.844$ & \\cellcolor{green!15} $0.72$ & \\cellcolor{green!15} $1.61$ & $80$ & \\cellcolor{green!15} $68$ & \\cellcolor{green!15} $72$ & \\cellcolor{green!15} $0.266$ \\\\\n",
       "  &  & xcodec & $4.38$ & $103.0$ & $13.1$ & $0.0222$ & $0.0100$ & $1.274$ & $0.65$ & $1.83$ & \\cellcolor{green!15} $81$ & $66$ & $71$ & $0.329$ \\\\\n",
       "\\hline\n",
       "\\hline\\hline\n",
       " \\textbf{Worst} & Classic Rock & dac & $6.33$ & $610.1$ & $3.3$ & $0.0234$ & \\cellcolor{green!15} $0.0132$ & $1.023$ & $0.62$ & $2.64$ & $72$ & \\cellcolor{green!15} $66$ & $66$ & $0.499$ \\\\\n",
       "  &  & encodec & \\cellcolor{green!15} $2.63$ & \\cellcolor{green!15} $19.3$ & \\cellcolor{green!15} $34.5$ & \\cellcolor{green!15} $0.0226$ & $0.0132$ & \\cellcolor{green!15} $0.788$ & \\cellcolor{green!15} $0.72$ & \\cellcolor{green!15} $1.18$ & $77$ & $65$ & \\cellcolor{green!15} $68$ & \\cellcolor{green!15} $0.197$ \\\\\n",
       "  &  & xcodec & $4.52$ & $112.8$ & $12.0$ & $0.0450$ & $0.0266$ & $2.177$ & $0.40$ & $2.82$ & \\cellcolor{green!15} $77$ & $61$ & $66$ & $0.455$ \\\\\n",
       "\\hline\n",
       " \\textbf{Worst} & Arena Stage & dac & $6.32$ & $593.7$ & $3.1$ & $0.0265$ & $0.0159$ & $1.153$ & $0.50$ & $2.65$ & $75$ & $68$ & $69$ & $0.489$ \\\\\n",
       "  &  & encodec & \\cellcolor{green!15} $2.59$ & \\cellcolor{green!15} $18.3$ & \\cellcolor{green!15} $34.5$ & \\cellcolor{green!15} $0.0241$ & \\cellcolor{green!15} $0.0146$ & \\cellcolor{green!15} $0.780$ & \\cellcolor{green!15} $0.70$ & \\cellcolor{green!15} $1.63$ & $78$ & \\cellcolor{green!15} $69$ & \\cellcolor{green!15} $70$ & \\cellcolor{green!15} $0.155$ \\\\\n",
       "  &  & xcodec & $4.44$ & $107.0$ & $12.2$ & $0.0404$ & $0.0247$ & $1.604$ & $0.49$ & $2.67$ & \\cellcolor{green!15} $79$ & $66$ & $69$ & $0.334$ \\\\\n",
       "\\hline\n",
       " \\textbf{Worst} & Ele-Drum & dac & $6.05$ & $483.3$ & $5.9$ & \\cellcolor{green!15} $0.0250$ & \\cellcolor{green!15} $0.0143$ & $0.986$ & $0.70$ & $1.62$ & $70$ & $67$ & $66$ & $0.325$ \\\\\n",
       "  &  & encodec & \\cellcolor{green!15} $2.46$ & \\cellcolor{green!15} $14.7$ & \\cellcolor{green!15} $35.9$ & $0.0262$ & $0.0149$ & \\cellcolor{green!15} $0.834$ & \\cellcolor{green!15} $0.75$ & \\cellcolor{green!15} $1.59$ & $74$ & $66$ & $67$ & $0.187$ \\\\\n",
       "  &  & xcodec & $4.27$ & $89.8$ & $12.9$ & $0.0581$ & $0.0355$ & $2.486$ & $0.54$ & $1.88$ & \\cellcolor{green!15} $76$ & \\cellcolor{green!15} $68$ & \\cellcolor{green!15} $70$ & \\cellcolor{green!15} $0.155$ \\\\\n",
       "\\hline\n",
       "\\end{tabular}%\n",
       "}\n",
       "\\vspace{1pt}\n",
       "\\parbox{\\linewidth}{\\scriptsize $^{\\mathrm{a}}$PAD ignored. $^{\\mathrm{b}}$Decoded audio at 32\\,kHz. $^{\\mathrm{c}}$Onsets: pred-audio vs grid GT within 50\\,ms (GT vel$>0.30$). $^{\\mathrm{d}}$Per-kit FAD from \\texttt{fadtk} (CLAP-LAION-music).}\n",
       "\\label{tab:top_worst_kits_allkits_compact}\n",
       "\\end{center}\n",
       "\\end{table*}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "K = 3\n",
    "OUT_DIR = Path(\"artifacts/eval/small_all_kits\")\n",
    "FAD_PER_KIT_CSV = Path(\"artifacts/eval/fadtk_per_kit_small_all_kits/fadtk_per_kit.csv\")\n",
    "\n",
    "TABLE_ENV = \"table*\"\n",
    "HICOLOR = r\"green!15\"\n",
    "DASH = r\"---\"\n",
    "\n",
    "# Compact col widths\n",
    "COLSPEC = (\n",
    "    r\"|p{0.055\\linewidth}|p{0.165\\linewidth}|p{0.055\\linewidth}|\"\n",
    "    r\"p{0.050\\linewidth}|p{0.050\\linewidth}|p{0.055\\linewidth}|\"\n",
    "    r\"p{0.060\\linewidth}|p{0.060\\linewidth}|p{0.070\\linewidth}|p{0.070\\linewidth}|p{0.070\\linewidth}|\"\n",
    "    r\"p{0.052\\linewidth}|p{0.052\\linewidth}|p{0.052\\linewidth}|\"\n",
    "    r\"p{0.055\\linewidth}|\"\n",
    ")\n",
    "\n",
    "def load_eval(out_dir: Path):\n",
    "    s = json.loads((out_dir / \"summary.json\").read_text(encoding=\"utf-8\"))\n",
    "    it = pd.read_csv(out_dir / \"items.csv\")\n",
    "    return s, it\n",
    "\n",
    "def base_systems(summary: dict) -> list[str]:\n",
    "    return [k for k in summary[\"systems\"].keys() if not k.endswith((\"_oracle\", \"_random\"))]\n",
    "\n",
    "def codec_label(sys_key: str) -> str:\n",
    "    s = str(sys_key).lower()\n",
    "    if s.startswith(\"encodec\"): return \"encodec\"\n",
    "    if s.startswith(\"xcodec\"):  return \"xcodec\"\n",
    "    if s.startswith(\"dac\"):     return \"dac\"\n",
    "    return s\n",
    "\n",
    "def H(tex: str) -> str:\n",
    "    return rf\"\\shortstack[c]{{\\scriptsize {tex}}}\"\n",
    "\n",
    "def mean_only(mu: float, digits: int, *, pct: bool=False) -> str:\n",
    "    if not np.isfinite(mu):\n",
    "        return DASH\n",
    "    if pct:\n",
    "        mu *= 100.0\n",
    "    return rf\"${mu:.{digits}f}$\"\n",
    "\n",
    "def fnum(v: float, digits: int = 3) -> str:\n",
    "    return DASH if not np.isfinite(v) else rf\"${float(v):.{digits}f}$\"\n",
    "\n",
    "summary, items = load_eval(OUT_DIR)\n",
    "bases = base_systems(summary)\n",
    "\n",
    "if \"token_nll\" not in items.columns:\n",
    "    raise RuntimeError(\"items.csv must contain token_nll\")\n",
    "\n",
    "items = items.copy()\n",
    "items[\"token_ppl\"] = np.exp(items[\"token_nll\"].astype(float))\n",
    "\n",
    "encodec_sys = next((b for b in bases if str(b).lower().startswith(\"encodec\")), None)\n",
    "if encodec_sys is None:\n",
    "    raise RuntimeError(f\"No EnCodec system in {bases}\")\n",
    "\n",
    "enc_kit_acc = (\n",
    "    items.loc[items[\"system\"] == encodec_sys]\n",
    "    .groupby(\"kit\")[\"token_acc\"].mean()\n",
    "    .sort_values(ascending=True)\n",
    ")\n",
    "worst_kits = enc_kit_acc.head(K).index.tolist()\n",
    "top_kits   = enc_kit_acc.tail(K).index.tolist()[::-1]\n",
    "selected_kits = top_kits + worst_kits\n",
    "\n",
    "METRICS = [\n",
    "    dict(key=\"token_nll\",        disp=r\"\\textbf{NLL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$\",  digits=2, dir=\"min\", pct=False),\n",
    "    dict(key=\"token_ppl\",        disp=r\"\\textbf{PPL}$^{\\mathrm{a}}$\\,\\,$\\downarrow$\",  digits=1, dir=\"min\", pct=False),\n",
    "    dict(key=\"token_acc\",        disp=r\"\\textbf{Acc(\\%)}$^{\\mathrm{a}}$\\,\\,$\\uparrow$\",digits=1, dir=\"max\", pct=True),\n",
    "\n",
    "    dict(key=\"rmse\",             disp=r\"\\textbf{RMSE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$\", digits=4, dir=\"min\", pct=False),\n",
    "    dict(key=\"mae\",              disp=r\"\\textbf{MAE}$^{\\mathrm{b}}$\\,\\,$\\downarrow$\",  digits=4, dir=\"min\", pct=False),\n",
    "    dict(key=\"mr_stft_sc\",       disp=r\"\\textbf{MRSTFT}$^{\\mathrm{b}}$\\,\\,$\\downarrow$\",digits=3, dir=\"min\", pct=False),\n",
    "    dict(key=\"env_rms_corr\",     disp=r\"\\textbf{Env}$^{\\mathrm{b}}$\\,\\,$\\uparrow$\",    digits=2, dir=\"max\", pct=False),\n",
    "    dict(key=\"tter_db_mae\",      disp=r\"\\textbf{TTER}$^{\\mathrm{b}}$\\,\\,$\\downarrow$\", digits=2, dir=\"min\", pct=False),\n",
    "\n",
    "    dict(key=\"onset_precision\",  disp=r\"\\textbf{P(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$\",  digits=0, dir=\"max\", pct=True),\n",
    "    dict(key=\"onset_recall\",     disp=r\"\\textbf{R(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$\",  digits=0, dir=\"max\", pct=True),\n",
    "    dict(key=\"onset_f1\",         disp=r\"\\textbf{F1(\\%)}$^{\\mathrm{c}}$\\,\\,$\\uparrow$\", digits=0, dir=\"max\", pct=True),\n",
    "]\n",
    "\n",
    "metric_keys = [m[\"key\"] for m in METRICS if m[\"key\"] in items.columns]\n",
    "METRICS = [m for m in METRICS if m[\"key\"] in items.columns]\n",
    "\n",
    "sub = items.loc[items[\"kit\"].isin(selected_kits) & items[\"system\"].isin(bases)].copy()\n",
    "g_mean = sub.groupby([\"kit\", \"system\"])[metric_keys].mean()\n",
    "\n",
    "fad_map = {}\n",
    "if FAD_PER_KIT_CSV.is_file():\n",
    "    fad_df = pd.read_csv(FAD_PER_KIT_CSV)\n",
    "    fad_df[\"kit\"] = fad_df[\"kit\"].astype(str)\n",
    "    fad_df[\"system\"] = fad_df[\"system\"].astype(str)\n",
    "    fad_map = {(r[\"kit\"], r[\"system\"]): float(r[\"fad\"]) for _, r in fad_df.iterrows()}\n",
    "HAS_FAD = any((kit, sys) in fad_map and np.isfinite(fad_map[(kit, sys)]) for kit in selected_kits for sys in bases)\n",
    "\n",
    "best_cells = set()\n",
    "for kit in selected_kits:\n",
    "    for m in METRICS:\n",
    "        vals = []\n",
    "        for sys in bases:\n",
    "            if (kit, sys) in g_mean.index:\n",
    "                v = float(g_mean.loc[(kit, sys), m[\"key\"]])\n",
    "                if np.isfinite(v):\n",
    "                    vals.append((sys, v))\n",
    "        if vals:\n",
    "            best_sys = min(vals, key=lambda t: t[1])[0] if m[\"dir\"] == \"min\" else max(vals, key=lambda t: t[1])[0]\n",
    "            best_cells.add((kit, best_sys, m[\"key\"]))\n",
    "if HAS_FAD:\n",
    "    for kit in selected_kits:\n",
    "        vals = [(sys, fad_map.get((kit, sys), np.nan)) for sys in bases]\n",
    "        vals = [(sys, float(v)) for (sys, v) in vals if np.isfinite(v)]\n",
    "        if vals:\n",
    "            best_cells.add((kit, min(vals, key=lambda t: t[1])[0], \"__fad__\"))\n",
    "\n",
    "def metric_cell(kit: str, sys: str, m: dict) -> str:\n",
    "    mu = float(g_mean.loc[(kit, sys), m[\"key\"]]) if (kit, sys) in g_mean.index else np.nan\n",
    "    out = mean_only(mu, m[\"digits\"], pct=m[\"pct\"])\n",
    "    if out != DASH and (kit, sys, m[\"key\"]) in best_cells:\n",
    "        return rf\"\\cellcolor{{{HICOLOR}}} {out}\"\n",
    "    return out\n",
    "\n",
    "def fad_cell(kit: str, sys: str) -> str:\n",
    "    v = fad_map.get((kit, sys), np.nan)\n",
    "    out = fnum(v, digits=3)\n",
    "    if out != DASH and (kit, sys, \"__fad__\") in best_cells:\n",
    "        return rf\"\\cellcolor{{{HICOLOR}}} {out}\"\n",
    "    return out\n",
    "\n",
    "caption = rf\"Top-{K} and worst-{K} kits by EnCodec token accuracy (all-kits). Best values per kit highlighted.\"\n",
    "label = \"tab:top_worst_kits_allkits_compact\"\n",
    "\n",
    "lines = []\n",
    "lines.append(rf\"\\begin{{{TABLE_ENV}}}[htbp]\")\n",
    "lines.append(rf\"\\caption{{{caption}}}\")\n",
    "lines.append(r\"\\begin{center}\")\n",
    "lines.append(r\"\\renewcommand{\\arraystretch}{0.95}\")\n",
    "lines.append(r\"\\setlength{\\tabcolsep}{0.6pt}\")\n",
    "lines.append(r\"\\scriptsize\")\n",
    "lines.append(r\"\\resizebox{\\textwidth}{!}{%\")\n",
    "lines.append(r\"\\begin{tabular}{\" + COLSPEC + r\"}\")\n",
    "lines.append(r\"\\hline\")\n",
    "\n",
    "lines.append(\n",
    "    r\"\\textbf{Eval} & \\textbf{Kit} & \\textbf{Codec} \"\n",
    "    r\"& \\multicolumn{3}{c|}{\\textbf{Token}} \"\n",
    "    r\"& \\multicolumn{5}{c|}{\\textbf{Audio}} \"\n",
    "    r\"& \\multicolumn{3}{c|}{\\textbf{Onset}} \"\n",
    "    r\"& \\\\\"\n",
    ")\n",
    "lines.append(r\"\\cline{4-6}\\cline{7-11}\\cline{12-14}\")\n",
    "\n",
    "lines.append(\n",
    "    r\"\\textbf{Set} & \\textbf{} & \\textbf{}\"\n",
    "    r\" & \" + H(METRICS[0][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[1][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[2][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[3][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[4][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[5][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[6][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[7][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[8][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[9][\"disp\"]) +\n",
    "    r\" & \" + H(METRICS[10][\"disp\"]) +\n",
    "    r\" & \" + H(r\"\\textbf{FAD}$^{\\mathrm{d}}$\\,\\,$\\downarrow$\") +\n",
    "    r\" \\\\\"\n",
    ")\n",
    "lines.append(r\"\\hline\")\n",
    "\n",
    "def emit_block(block_name, kits):\n",
    "    for kit in kits:\n",
    "        first = True\n",
    "        for sys in bases:\n",
    "            row = [rf\"\\textbf{{{block_name}}}\" if first else \"\", kit if first else \"\", codec_label(sys)]\n",
    "            for m in METRICS:\n",
    "                row.append(metric_cell(kit, sys, m))\n",
    "            row.append(fad_cell(kit, sys) if HAS_FAD else DASH)\n",
    "            lines.append(\" \" + \" & \".join(row) + r\" \\\\\")\n",
    "            first = False\n",
    "        lines.append(r\"\\hline\")\n",
    "\n",
    "emit_block(\"Top\", top_kits)\n",
    "lines.append(r\"\\hline\\hline\")\n",
    "emit_block(\"Worst\", worst_kits)\n",
    "\n",
    "lines.append(r\"\\end{tabular}%\")\n",
    "lines.append(r\"}\")  # resizebox\n",
    "lines.append(r\"\\vspace{1pt}\")\n",
    "lines.append(\n",
    "    r\"\\parbox{\\linewidth}{\\scriptsize \"\n",
    "    r\"$^{\\mathrm{a}}$PAD ignored. \"\n",
    "    r\"$^{\\mathrm{b}}$Decoded audio at 32\\,kHz. \"\n",
    "    r\"$^{\\mathrm{c}}$Onsets: pred-audio vs grid GT within 50\\,ms (GT vel$>0.30$). \"\n",
    "    r\"$^{\\mathrm{d}}$Per-kit FAD from \\texttt{fadtk} (CLAP-LAION-music).}\"\n",
    ")\n",
    "lines.append(rf\"\\label{{{label}}}\")\n",
    "lines.append(r\"\\end{center}\")\n",
    "lines.append(rf\"\\end{{{TABLE_ENV}}}\")\n",
    "\n",
    "display(Markdown(\"```latex\\n\" + \"\\n\".join(lines) + \"\\n```\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507e0dd",
   "metadata": {},
   "source": [
    "# Eval metrics viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-example onset-metrics debug (GT = cached GRID onsets w/ filtering) + drum grid plot + audio playback\n",
    "# GT excludes hh_closed + low-velocity hits; preds from audio onsets (madmom if available); ±50ms eval.\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, hashlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import importlib\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "os.environ.setdefault(\"MIDIGROOVE_ONSET_BACKEND\", \"madmom\")  # \"madmom\" | \"native\"\n",
    "\n",
    "from midigroove_poc import eval as mg_eval\n",
    "importlib.reload(mg_eval)\n",
    "\n",
    "PRED_RUN_DIR = Path(\"artifacts/pred/small_one_kit\")\n",
    "CACHE_DIR = Path(\"cache/encodec_acoustic\")\n",
    "SYSTEMS = [\"encodec\", \"xcodec\", \"dac\"]\n",
    "\n",
    "TOL_MS = 50.0\n",
    "MAX_SECONDS_TO_SHOW = None\n",
    "\n",
    "ONSET_KW = dict(min_separation_s=0.05, backtrack_ms=0.0, refine_ms=12.0, rms_gate_db=35.0)\n",
    "\n",
    "GT_VEL_THRESH = 0.30\n",
    "GT_EXCLUDE = []\n",
    "\n",
    "def read_wav_mono_float32(path: Path):\n",
    "    with wave.open(str(path), \"rb\") as wf:\n",
    "        sr = wf.getframerate()\n",
    "        n = wf.getnframes()\n",
    "        sampwidth = wf.getsampwidth()\n",
    "        nch = wf.getnchannels()\n",
    "        x = wf.readframes(n)\n",
    "    if sampwidth != 2:\n",
    "        raise RuntimeError(f\"Expected 16-bit PCM wav, got sampwidth={sampwidth} at {path}\")\n",
    "    y = np.frombuffer(x, dtype=np.int16).astype(np.float32) / 32767.0\n",
    "    if nch > 1:\n",
    "        y = y.reshape(-1, nch).mean(axis=1)\n",
    "    return y, int(sr)\n",
    "\n",
    "def stable_key_str(audio_path: str, midi_path: str, sr: int, start_sample: int, window_samples: int) -> str:\n",
    "    return hashlib.sha1(f\"{audio_path}|{midi_path}|{sr}|{start_sample}|{window_samples}\".encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def vlines(ax, samps, *, sr, color, alpha, lw, label=None):\n",
    "    if samps is None or len(samps) == 0:\n",
    "        return\n",
    "    xs = np.asarray(samps, dtype=np.float64) / float(sr)\n",
    "    ax.vlines(xs, ymin=ax.get_ylim()[0], ymax=ax.get_ylim()[1], color=color, alpha=alpha, linewidth=lw, label=label)\n",
    "\n",
    "def shade_tol(ax, ref_samps, *, sr, tol_samps, color=\"green\", alpha=0.06):\n",
    "    for r in np.asarray(ref_samps, dtype=np.int64):\n",
    "        t0 = (r - tol_samps) / float(sr)\n",
    "        t1 = (r + tol_samps) / float(sr)\n",
    "        ax.axvspan(t0, t1, color=color, alpha=alpha, linewidth=0)\n",
    "\n",
    "def clip_to_seconds(y, sr, sec):\n",
    "    if sec is None:\n",
    "        return y\n",
    "    n = int(min(y.size, round(float(sec) * float(sr))))\n",
    "    return y[:n]\n",
    "\n",
    "# --- pick a saved item present for all systems ---\n",
    "avail = []\n",
    "for sys in SYSTEMS:\n",
    "    ids = {p.stem for p in (PRED_RUN_DIR / \"pred\" / sys).glob(\"*.wav\")}\n",
    "    if not ids:\n",
    "        raise RuntimeError(f\"No saved preds in {PRED_RUN_DIR/'pred'/sys}\")\n",
    "    avail.append(ids)\n",
    "item_id = sorted(set.intersection(*avail))[0]\n",
    "print(\"Using saved item_id:\", item_id)\n",
    "\n",
    "ref_wav_path = PRED_RUN_DIR / \"ref\" / f\"{item_id}.wav\"\n",
    "if not ref_wav_path.is_file():\n",
    "    raise FileNotFoundError(f\"Missing ref wav: {ref_wav_path}\")\n",
    "\n",
    "summary = json.loads(Path(\"artifacts/eval/small_one_kit/summary.json\").read_text(encoding=\"utf-8\"))\n",
    "eval_sr = int(summary[\"eval_sr\"])\n",
    "tol_samps = max(1, int(round((TOL_MS * 1e-3) * eval_sr)))\n",
    "print(\"eval_sr:\", eval_sr, \"tol_ms:\", TOL_MS, \"tol_samps:\", tol_samps)\n",
    "\n",
    "# --- find matching npz in cache test manifest, load grid for visualization ---\n",
    "manifest = sorted(CACHE_DIR.glob(\"manifest_midigroove_test_*.jsonl\"))[0]\n",
    "npz_match = None\n",
    "drum_hit = drum_vel = None\n",
    "window_sec = None\n",
    "\n",
    "for line in manifest.read_text(encoding=\"utf-8\").splitlines():\n",
    "    rec = json.loads(line)\n",
    "    npz = Path(rec[\"npz\"])\n",
    "    with np.load(npz, allow_pickle=False) as d:\n",
    "        audio_path = str(d[\"audio_path\"].item())\n",
    "        midi_p = str(d[\"midi_path\"].item())\n",
    "        sr0 = int(d[\"sr\"].item())\n",
    "        st = float(d[\"start_sec\"].item())\n",
    "        ws = float(d[\"window_seconds\"].item())\n",
    "        ss = int(round(st * sr0))\n",
    "        ns = int(round(ws * sr0))\n",
    "        if stable_key_str(audio_path, midi_p, sr0, ss, ns) != item_id:\n",
    "            continue\n",
    "        npz_match = npz\n",
    "        window_sec = float(ws)\n",
    "        drum_hit = np.asarray(d[\"drum_hit\"], dtype=np.float32)\n",
    "        drum_vel = np.asarray(d[\"drum_vel\"], dtype=np.float32) if \"drum_vel\" in d else np.zeros_like(drum_hit)\n",
    "        break\n",
    "\n",
    "if npz_match is None:\n",
    "    raise RuntimeError(f\"Could not find item_id={item_id} in {manifest}\")\n",
    "print(\"npz:\", npz_match.name, \"D,T:\", drum_hit.shape, \"window_sec:\", window_sec)\n",
    "\n",
    "# --- load reference audio ---\n",
    "y_ref, sr_ref = read_wav_mono_float32(ref_wav_path)\n",
    "assert sr_ref == eval_sr\n",
    "\n",
    "# GT from GRID (filtered)\n",
    "gt_onsets = mg_eval._onsets_from_grid_npz(\n",
    "    npz_match,\n",
    "    eval_sr=eval_sr,\n",
    "    vel_thresh=GT_VEL_THRESH,\n",
    "    exclude_channels=GT_EXCLUDE,\n",
    ")\n",
    "print(\"grid_gt_onsets(filtered):\", len(gt_onsets))\n",
    "\n",
    "# optional overlay: ref-audio detected onsets (NOT GT)\n",
    "ref_audio_onsets = mg_eval._onsets_from_audio(y_ref, sr=eval_sr, **ONSET_KW)\n",
    "\n",
    "# --- per-system pred onsets + scores vs GRID GT ---\n",
    "pred_audio, pred_onsets, scores = {}, {}, {}\n",
    "for sys in SYSTEMS:\n",
    "    y, sr = read_wav_mono_float32(PRED_RUN_DIR / \"pred\" / sys / f\"{item_id}.wav\")\n",
    "    assert sr == eval_sr\n",
    "    pred_audio[sys] = y\n",
    "    pred_onsets[sys] = mg_eval._onsets_from_audio(y, sr=eval_sr, **ONSET_KW)\n",
    "\n",
    "    m = mg_eval._onset_pr_metrics(\n",
    "        pred=y,\n",
    "        ref=y_ref,\n",
    "        sr=eval_sr,\n",
    "        pred_onsets=pred_onsets[sys],\n",
    "        ref_onsets=gt_onsets,\n",
    "        midi_path=None, start_sec=None, end_sec=None,\n",
    "        tol_ms=TOL_MS,\n",
    "    )\n",
    "    scores[sys] = m\n",
    "    print(f\"{sys:7s} P={m['onset_precision']:.3f} R={m['onset_recall']:.3f} F1={m['onset_f1']:.3f} (pred={len(pred_onsets[sys])} gt={len(gt_onsets)})\")\n",
    "\n",
    "print(\"\\nAudio: reference then each system prediction\")\n",
    "display(Audio(clip_to_seconds(y_ref, eval_sr, MAX_SECONDS_TO_SHOW), rate=eval_sr))\n",
    "for sys in SYSTEMS:\n",
    "    display(Audio(clip_to_seconds(pred_audio[sys], eval_sr, MAX_SECONDS_TO_SHOW), rate=eval_sr))\n",
    "\n",
    "t = np.arange(y_ref.size) / float(eval_sr)\n",
    "n = y_ref.size if MAX_SECONDS_TO_SHOW is None else int(min(y_ref.size, round(float(MAX_SECONDS_TO_SHOW) * eval_sr)))\n",
    "\n",
    "fig, axes = plt.subplots(2 + len(SYSTEMS), 1, figsize=(14, 2.0 * (2 + len(SYSTEMS))), sharex=True)\n",
    "\n",
    "ax0 = axes[0]\n",
    "D, T = drum_hit.shape\n",
    "grid_img = np.concatenate([drum_hit, drum_vel], axis=0)\n",
    "ax0.imshow(grid_img, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
    "           extent=[0.0, float(window_sec), 0.0, float(2 * D)])\n",
    "ax0.set_title(f\"Cached grid + GT onsets (green) [exclude={GT_EXCLUDE}, vel>={GT_VEL_THRESH}]\")\n",
    "shade_tol(ax0, gt_onsets, sr=eval_sr, tol_samps=tol_samps, color=\"green\", alpha=0.05)\n",
    "vlines(ax0, gt_onsets, sr=eval_sr, color=\"green\", alpha=0.85, lw=1.2, label=\"GRID GT (filtered)\")\n",
    "ax0.legend(loc=\"upper right\", frameon=False)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(t[:n], y_ref[:n], color=\"0.25\", linewidth=0.9)\n",
    "ax.set_title(\"Reference waveform + GT (green) + ref-audio onsets (gray, optional)\")\n",
    "shade_tol(ax, gt_onsets, sr=eval_sr, tol_samps=tol_samps, color=\"green\", alpha=0.06)\n",
    "vlines(ax, gt_onsets, sr=eval_sr, color=\"green\", alpha=0.85, lw=1.2, label=\"GRID GT (filtered)\")\n",
    "vlines(ax, ref_audio_onsets, sr=eval_sr, color=\"0.6\", alpha=0.35, lw=1.0, label=\"ref audio onsets\")\n",
    "ax.legend(loc=\"upper right\", frameon=False)\n",
    "\n",
    "for i, sys in enumerate(SYSTEMS, 2):\n",
    "    ax = axes[i]\n",
    "    y = pred_audio[sys]\n",
    "    m = scores[sys]\n",
    "    ax.plot(t[:n], y[:n], color=\"C0\", linewidth=0.9)\n",
    "    ax.set_title(f\"{sys}: pred onsets (red) vs GT (green);  P={m['onset_precision']:.3f} R={m['onset_recall']:.3f} F1={m['onset_f1']:.3f}\")\n",
    "    shade_tol(ax, gt_onsets, sr=eval_sr, tol_samps=tol_samps, color=\"green\", alpha=0.05)\n",
    "    vlines(ax, gt_onsets, sr=eval_sr, color=\"green\", alpha=0.50, lw=1.0, label=\"GT\")\n",
    "    vlines(ax, pred_onsets[sys], sr=eval_sr, color=\"red\", alpha=0.70, lw=1.2, label=\"pred\")\n",
    "    if i == 2:\n",
    "        ax.legend(loc=\"upper right\", frameon=False)\n",
    "\n",
    "axes[-1].set_xlabel(\"seconds\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba70ff",
   "metadata": {},
   "source": [
    "# FAD check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5819c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for FAD(PANN): does it separate obvious wrong audio?\n",
    "# - FAD(ref, ref) should be ~0\n",
    "# - FAD(ref, pred) should be > 0\n",
    "# - FAD(ref, shuffled_pred) should be >= FAD(ref, pred)\n",
    "# - FAD(ref, noise) should be much larger than ref/pred\n",
    "# - FAD(ref, oracle) should usually be <= FAD(ref, pred)\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from frechet_audio_distance import FrechetAudioDistance\n",
    "\n",
    "RUN = Path(\"artifacts/pred/small_one_kit\")  # <-- change\n",
    "SYS = \"encodec\"  # <-- change: \"dac\" | \"encodec\" | \"xcodec\"\n",
    "EVAL_SR = 32000  # <-- must match what eval saved\n",
    "\n",
    "ref_dir    = RUN / \"ref_by_system\" / SYS\n",
    "pred_dir   = RUN / \"pred\" / SYS\n",
    "oracle_dir = RUN / \"oracle\" / SYS\n",
    "\n",
    "assert ref_dir.is_dir(), ref_dir\n",
    "assert pred_dir.is_dir(), pred_dir\n",
    "\n",
    "fad = FrechetAudioDistance(\n",
    "    model_name=\"pann\",\n",
    "    sample_rate=int(EVAL_SR),\n",
    "    use_pca=False,\n",
    "    use_activation=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "def _list_wavs(d: Path):\n",
    "    return sorted([p for p in d.rglob(\"*.wav\") if p.is_file()])\n",
    "\n",
    "def _make_clean_dir(d: Path):\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _copy_subset(src: Path, dst: Path, wavs: list[Path]):\n",
    "    _make_clean_dir(dst)\n",
    "    for p in wavs:\n",
    "        rel = p.relative_to(src)\n",
    "        out = dst / rel\n",
    "        out.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(p, out)\n",
    "\n",
    "def _shuffled_pred_subset(ref_wavs: list[Path], pred_wavs: list[Path], dst: Path):\n",
    "    # copy preds but with filenames matching refs (pairing wrong items)\n",
    "    _make_clean_dir(dst)\n",
    "    pred_shuf = pred_wavs[:]\n",
    "    random.shuffle(pred_shuf)\n",
    "    for r, p in zip(ref_wavs, pred_shuf):\n",
    "        out = dst / r.name  # flatten OK for FAD\n",
    "        shutil.copy2(p, out)\n",
    "\n",
    "def _noise_like_ref_subset(ref_wavs: list[Path], dst: Path):\n",
    "    # white noise with same length as each ref file (requires soundfile)\n",
    "    import soundfile as sf\n",
    "    _make_clean_dir(dst)\n",
    "    rng = np.random.default_rng(0)\n",
    "    for r in ref_wavs:\n",
    "        y, sr = sf.read(r, dtype=\"float32\", always_2d=False)\n",
    "        assert int(sr) == int(EVAL_SR), (sr, EVAL_SR, r)\n",
    "        if y.ndim > 1:\n",
    "            y = y.mean(axis=-1)\n",
    "        n = rng.standard_normal(size=y.shape[0]).astype(\"float32\")\n",
    "        # match RMS roughly\n",
    "        rms_y = float(np.sqrt(np.mean(np.square(y)) + 1e-12))\n",
    "        rms_n = float(np.sqrt(np.mean(np.square(n)) + 1e-12))\n",
    "        n = n * (rms_y / max(1e-8, rms_n))\n",
    "        sf.write(dst / r.name, n, int(EVAL_SR))\n",
    "\n",
    "def score(a: Path, b: Path) -> float:\n",
    "    return float(fad.score(str(a), str(b), dtype=\"float32\"))\n",
    "\n",
    "# ---- run checks on a subset for speed ----\n",
    "ref_wavs  = _list_wavs(ref_dir)\n",
    "pred_wavs = _list_wavs(pred_dir)\n",
    "assert len(ref_wavs) == len(pred_wavs), (len(ref_wavs), len(pred_wavs))\n",
    "\n",
    "N = min(512, len(ref_wavs))  # <-- increase for stability\n",
    "random.seed(0)\n",
    "idx = random.sample(range(len(ref_wavs)), k=N)\n",
    "ref_sub  = [ref_wavs[i] for i in idx]\n",
    "pred_sub = [pred_wavs[i] for i in idx]\n",
    "\n",
    "TMP = RUN / \"fad_sanity_tmp\" / SYS\n",
    "ref_tmp     = TMP / \"ref\"\n",
    "pred_tmp    = TMP / \"pred\"\n",
    "shuf_tmp    = TMP / \"pred_shuffled\"\n",
    "noise_tmp   = TMP / \"noise\"\n",
    "oracle_tmp  = TMP / \"oracle\"\n",
    "\n",
    "_copy_subset(ref_dir, ref_tmp, ref_sub)\n",
    "_copy_subset(pred_dir, pred_tmp, pred_sub)\n",
    "_shuffled_pred_subset(ref_sub, pred_sub, shuf_tmp)\n",
    "_noise_like_ref_subset(ref_sub, noise_tmp)\n",
    "\n",
    "print(f\"Subset N={N} @ sr={EVAL_SR} sys={SYS}\")\n",
    "fad_ref_ref   = score(ref_tmp, ref_tmp)\n",
    "fad_ref_pred  = score(ref_tmp, pred_tmp)\n",
    "fad_ref_shuf  = score(ref_tmp, shuf_tmp)\n",
    "fad_ref_noise = score(ref_tmp, noise_tmp)\n",
    "\n",
    "print(f\"FAD(ref, ref)        = {fad_ref_ref:.8e}\")\n",
    "print(f\"FAD(ref, pred)       = {fad_ref_pred:.8e}\")\n",
    "print(f\"FAD(ref, shuffled)   = {fad_ref_shuf:.8e}\")\n",
    "print(f\"FAD(ref, noise)      = {fad_ref_noise:.8e}\")\n",
    "\n",
    "if oracle_dir.is_dir():\n",
    "    oracle_wavs = _list_wavs(oracle_dir)\n",
    "    if len(oracle_wavs) >= N:\n",
    "        oracle_sub = [oracle_wavs[i] for i in idx]\n",
    "        _copy_subset(oracle_dir, oracle_tmp, oracle_sub)\n",
    "        fad_ref_oracle = score(ref_tmp, oracle_tmp)\n",
    "        print(f\"FAD(ref, oracle)     = {fad_ref_oracle:.8e}\")\n",
    "    else:\n",
    "        print(\"Oracle present but not enough wavs for subset.\")\n",
    "\n",
    "print(\"Tmp written to:\", TMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: check if FAD is mostly driven by loudness (RMS) mismatch\n",
    "# (If FAD barely changes after RMS-normalizing, it’s not sensitive to your errors.)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import shutil\n",
    "import soundfile as sf\n",
    "from frechet_audio_distance import FrechetAudioDistance\n",
    "\n",
    "RUN = Path(\"artifacts/pred/small_one_kit\")  # <-- change\n",
    "SYS = \"encodec\"\n",
    "EVAL_SR = 32000\n",
    "\n",
    "ref_dir  = RUN / \"ref_by_system\" / SYS\n",
    "pred_dir = RUN / \"pred\" / SYS\n",
    "\n",
    "fad = FrechetAudioDistance(model_name=\"pann\", sample_rate=int(EVAL_SR), use_pca=False, use_activation=False, verbose=False)\n",
    "\n",
    "def _list_wavs(d: Path):\n",
    "    return sorted([p for p in d.rglob(\"*.wav\") if p.is_file()])\n",
    "\n",
    "def _make_clean_dir(d: Path):\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _rms_normalize_tree(src: Path, dst: Path, target_rms: float = 0.05):\n",
    "    _make_clean_dir(dst)\n",
    "    for p in _list_wavs(src):\n",
    "        y, sr = sf.read(p, dtype=\"float32\", always_2d=False)\n",
    "        assert int(sr) == int(EVAL_SR), (sr, EVAL_SR, p)\n",
    "        if y.ndim > 1:\n",
    "            y = y.mean(axis=-1)\n",
    "        rms = float(np.sqrt(np.mean(np.square(y)) + 1e-12))\n",
    "        y = y * (float(target_rms) / max(1e-8, rms))\n",
    "        out = dst / p.name\n",
    "        sf.write(out, y, int(EVAL_SR))\n",
    "\n",
    "TMP = RUN / \"fad_rms_tmp\" / SYS\n",
    "ref_n = TMP / \"ref_norm\"\n",
    "pred_n = TMP / \"pred_norm\"\n",
    "\n",
    "ref = RUN / \"fad_sanity_tmp\" / SYS / \"ref\"     # reuse subset from previous snippet if present\n",
    "pred = RUN / \"fad_sanity_tmp\" / SYS / \"pred\"\n",
    "\n",
    "if not ref.is_dir():\n",
    "    ref = ref_dir\n",
    "if not pred.is_dir():\n",
    "    pred = pred_dir\n",
    "\n",
    "_rms_normalize_tree(ref, ref_n, target_rms=0.05)\n",
    "_rms_normalize_tree(pred, pred_n, target_rms=0.05)\n",
    "\n",
    "fad_raw = float(fad.score(str(ref), str(pred), dtype=\"float32\"))\n",
    "fad_norm = float(fad.score(str(ref_n), str(pred_n), dtype=\"float32\"))\n",
    "\n",
    "print(f\"FAD raw  = {fad_raw:.8e}\")\n",
    "print(f\"FAD norm = {fad_norm:.8e}\")\n",
    "print(\"Tmp written to:\", TMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4376139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Verify the \"noise\" files are actually noise (not accidentally refs)\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "RUN = Path(\"artifacts/pred/small_one_kit\")\n",
    "SYS = \"encodec\"\n",
    "ref_dir  = RUN / \"fad_sanity_tmp\" / SYS / \"ref\"\n",
    "noise_dir = RUN / \"fad_sanity_tmp\" / SYS / \"noise\"\n",
    "\n",
    "p = sorted(ref_dir.glob(\"*.wav\"))[0]\n",
    "r, sr = sf.read(p, dtype=\"float32\")\n",
    "n, sr2 = sf.read(noise_dir / p.name, dtype=\"float32\")\n",
    "m = min(len(r), len(n))\n",
    "r = r[:m]; n = n[:m]\n",
    "\n",
    "print(\"sr:\", sr, sr2)\n",
    "print(\"rms ref  :\", float(np.sqrt(np.mean(r*r) + 1e-12)))\n",
    "print(\"rms noise:\", float(np.sqrt(np.mean(n*n) + 1e-12)))\n",
    "print(\"corr(ref,noise):\", float(np.corrcoef(r, n)[0,1]))\n",
    "print(\"max|ref-noise|:\", float(np.max(np.abs(r - n))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Make more extreme baselines: silence + full-scale noise (no RMS matching)\n",
    "#    Expect: FAD(ref, silence) and FAD(ref, loud_noise) >> FAD(ref, pred)\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from frechet_audio_distance import FrechetAudioDistance\n",
    "\n",
    "RUN = Path(\"artifacts/pred/small_one_kit\")\n",
    "SYS = \"encodec\"\n",
    "EVAL_SR = 32000\n",
    "\n",
    "ref_tmp = RUN / \"fad_sanity_tmp\" / SYS / \"ref\"\n",
    "sil_dir = RUN / \"fad_sanity_tmp\" / SYS / \"silence\"\n",
    "ln_dir  = RUN / \"fad_sanity_tmp\" / SYS / \"loud_noise\"\n",
    "\n",
    "def clean(d: Path):\n",
    "    if d.exists(): shutil.rmtree(d)\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clean(sil_dir); clean(ln_dir)\n",
    "\n",
    "for p in sorted(ref_tmp.glob(\"*.wav\")):\n",
    "    y, sr = sf.read(p, dtype=\"float32\")\n",
    "    assert int(sr) == int(EVAL_SR)\n",
    "    sf.write(sil_dir / p.name, np.zeros_like(y), int(EVAL_SR))\n",
    "    # loud full-scale noise (no RMS match)\n",
    "    rng = np.random.default_rng(0)\n",
    "    n = rng.standard_normal(size=y.shape).astype(\"float32\")\n",
    "    n = n / (np.max(np.abs(n)) + 1e-8) * 0.95\n",
    "    sf.write(ln_dir / p.name, n, int(EVAL_SR))\n",
    "\n",
    "fad = FrechetAudioDistance(model_name=\"pann\", sample_rate=int(EVAL_SR), use_pca=False, use_activation=False, verbose=False)\n",
    "print(\"FAD(ref, silence)   =\", float(fad.score(str(ref_tmp), str(sil_dir), dtype=\"float32\")))\n",
    "print(\"FAD(ref, loud_noise)=\", float(fad.score(str(ref_tmp), str(ln_dir), dtype=\"float32\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "CACHE = Path(\"cache/dac_allkits\")\n",
    "bad = []\n",
    "for p in sorted((CACHE/\"items\").glob(\"*.npz\")):\n",
    "    try:\n",
    "        with np.load(p, allow_pickle=False) as d:\n",
    "            _ = d.files\n",
    "    except Exception as e:\n",
    "        if isinstance(e, zipfile.BadZipFile) or \"BadZipFile\" in str(e):\n",
    "            bad.append(p)\n",
    "print(\"bad:\", len(bad))\n",
    "for p in bad[:50]:\n",
    "    print(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "CACHE = Path(\"cache/dac_allkits\")\n",
    "bad = []\n",
    "for p in sorted((CACHE/\"items\").glob(\"*.npz\")):\n",
    "    try:\n",
    "        with np.load(p, allow_pickle=False) as d:\n",
    "            _ = d.files\n",
    "    except Exception as e:\n",
    "        if isinstance(e, zipfile.BadZipFile) or \"BadZipFile\" in str(e):\n",
    "            bad.append(p)\n",
    "for p in bad:\n",
    "    p.unlink(missing_ok=True)\n",
    "print(\"deleted:\", len(bad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a65cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether PAD->0 before decoding fixes “noisy” big-model preds.\n",
    "# Plays A/B audio (raw decode vs PAD-mapped decode) for a few items.\n",
    "\n",
    "from pathlib import Path\n",
    "import json, numpy as np, torch\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from midigroove_poc import expressivegrid as eg\n",
    "from midigroove_poc.eval import _load_audio_segment, _resample_linear\n",
    "from data.codecs import decode_tokens_to_audio\n",
    "\n",
    "CACHE_DIR = Path(\"cache/encodec_acoustic\")  # use the cache matching the system you’re testing\n",
    "CKPT_PATH = Path(\"artifacts/checkpoints/encodec_big_single_kit.pt\")\n",
    "DEVICE = \"cuda:0\"\n",
    "DECODE_DEVICE = \"cuda:0\"\n",
    "EVAL_SR = 32000\n",
    "N_EX = 5  # how many examples to audition\n",
    "\n",
    "# --- load a few test items ---\n",
    "manifest = sorted(CACHE_DIR.glob(\"manifest_midigroove_test_*.jsonl\"))[0]\n",
    "recs = [json.loads(x) for x in manifest.read_text().splitlines()[:N_EX]]\n",
    "npz_paths = [Path(r[\"npz\"]) for r in recs]\n",
    "print(\"NPZs:\", [p.name for p in npz_paths])\n",
    "\n",
    "# --- load ckpt + build model ---\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "state = ckpt[\"model\"]\n",
    "cfg = ckpt.get(\"cfg\", {}) if isinstance(ckpt.get(\"cfg\", {}), dict) else {}\n",
    "num_codebooks = int(ckpt[\"num_codebooks\"])\n",
    "in_dim = int(ckpt[\"in_dim\"])\n",
    "codec = str(cfg.get(\"encoder_model\", \"encodec\") or \"encodec\").strip().lower()\n",
    "\n",
    "# resolve vocab/pad exactly like eval does\n",
    "cfg2 = dict(cfg)\n",
    "cfg2[\"in_dim\"] = in_dim\n",
    "cfg2[\"num_codebooks\"] = num_codebooks\n",
    "if \"use_kit_name\" not in cfg2:\n",
    "    cfg2[\"use_kit_name\"] = bool(\"kit_name_emb.weight\" in state)\n",
    "if \"vocab_size\" not in cfg2:\n",
    "    vs = eg._infer_vocab_size_from_state_dict(state, num_codebooks=num_codebooks)\n",
    "    if vs is not None:\n",
    "        cfg2[\"vocab_size\"] = int(vs)\n",
    "if \"vocab_size\" in cfg2 and (\"pad_id\" not in cfg2 or \"codebook_size\" not in cfg2):\n",
    "    vs2 = int(cfg2.get(\"vocab_size\", 0) or 0)\n",
    "    if vs2 > 1:\n",
    "        cfg2.setdefault(\"pad_id\", vs2 - 1)\n",
    "        cfg2.setdefault(\"codebook_size\", vs2 - 1)\n",
    "\n",
    "codebook_size = int(cfg2.get(\"codebook_size\", eg._default_codebook_size_for_encoder(codec)))\n",
    "pad_id = int(cfg2.get(\"pad_id\", eg._pad_id_for_codebook(codebook_size)))\n",
    "vocab_size = int(cfg2.get(\"vocab_size\", eg._vocab_size_for_codebook(codebook_size)))\n",
    "cfg2[\"codebook_size\"] = codebook_size\n",
    "cfg2[\"pad_id\"] = pad_id\n",
    "cfg2[\"vocab_size\"] = vocab_size\n",
    "\n",
    "print(\"codec:\", codec, \"codebook_size:\", codebook_size, \"pad_id:\", pad_id, \"vocab_size:\", vocab_size)\n",
    "\n",
    "model = eg._build_model(num_codebooks=num_codebooks, in_dim=in_dim, cfg=cfg2)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.to(torch.device(DEVICE)).eval()\n",
    "\n",
    "# helper: build grid the same way training does\n",
    "def load_item(npz_path: Path):\n",
    "    with np.load(npz_path, allow_pickle=False) as d:\n",
    "        ex = {k: np.asarray(d[k]) for k in d.files}\n",
    "    drum_hit = ex[\"drum_hit\"].astype(np.float32)\n",
    "    drum_vel = ex.get(\"drum_vel\", np.zeros_like(drum_hit)).astype(np.float32)\n",
    "    drum_sus = ex.get(\"drum_sustain\", np.zeros_like(drum_hit)).astype(np.float32)\n",
    "    hh_cc4   = ex.get(\"hh_open_cc4\", np.zeros((drum_hit.shape[1],), np.float32)).astype(np.float32)\n",
    "    beat_pos = ex[\"beat_pos\"].astype(np.int64)\n",
    "    bpm = float(ex.get(\"bpm\", 120.0))\n",
    "    drummer_id = int(ex.get(\"drummer_id\", 0))\n",
    "    kit_name_id = int(ex.get(\"kit_name_id\", 0))\n",
    "    tgt = ex[\"tgt\"].astype(np.int64)\n",
    "\n",
    "    include_sustain = bool(cfg2.get(\"include_sustain\", False))\n",
    "    include_hh_cc4  = bool(cfg2.get(\"include_hh_cc4\", False))\n",
    "\n",
    "    pieces = [drum_hit, drum_vel]\n",
    "    if include_sustain:\n",
    "        pieces.append(drum_sus)\n",
    "    if include_hh_cc4:\n",
    "        pieces.append(hh_cc4[None, :])\n",
    "    grid = np.concatenate(pieces, axis=0).astype(np.float32)\n",
    "\n",
    "    # reference audio segment\n",
    "    audio_path = Path(str(ex[\"audio_path\"].item()))\n",
    "    sr_native = int(ex[\"sr\"].item())\n",
    "    start_sec = float(ex[\"start_sec\"].item())\n",
    "    window_seconds = float(ex[\"window_seconds\"].item())\n",
    "    start_sample = int(round(start_sec * sr_native))\n",
    "    window_samples = int(round(window_seconds * sr_native))\n",
    "    ref, sr_ref = _load_audio_segment(audio_path, start_sample=start_sample, num_samples=window_samples)\n",
    "    ref_rs = _resample_linear(ref, sr_ref, EVAL_SR)\n",
    "\n",
    "    return grid, beat_pos, bpm, drummer_id, kit_name_id, tgt, ref_rs\n",
    "\n",
    "for i, p in enumerate(npz_paths):\n",
    "    grid, beat_pos, bpm, drummer_id, kit_name_id, tgt, ref_rs = load_item(p)\n",
    "    T = int(grid.shape[1])\n",
    "\n",
    "    grid_t = torch.from_numpy(grid).unsqueeze(0).to(DEVICE)                 # [1,F,T]\n",
    "    beat_t = torch.from_numpy(beat_pos).unsqueeze(0).to(DEVICE)             # [1,T]\n",
    "    bpm_t  = torch.tensor([bpm], dtype=torch.float32, device=DEVICE)\n",
    "    dr_t   = torch.tensor([drummer_id], dtype=torch.long, device=DEVICE)\n",
    "    kit_t  = torch.tensor([kit_name_id], dtype=torch.long, device=DEVICE)\n",
    "    valid  = torch.ones((1, T), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logits = model(grid=grid_t, beat_pos=beat_t, bpm=bpm_t, drummer_id=dr_t,\n",
    "                       kit_name_id=kit_t if bool(cfg2.get(\"use_kit_name\", True)) else None,\n",
    "                       valid_mask=valid)\n",
    "        pred = logits.argmax(dim=-1).squeeze(0).to(torch.long).cpu()        # [C,T]\n",
    "\n",
    "    pad_rate = float((pred == pad_id).float().mean().item())\n",
    "    pred_raw = pred\n",
    "    pred_fix = torch.where(pred == pad_id, torch.zeros_like(pred), pred)\n",
    "\n",
    "    audio_raw_b1, sr_raw = decode_tokens_to_audio(pred_raw, encoder_model=codec, device=DECODE_DEVICE)\n",
    "    audio_fix_b1, sr_fix = decode_tokens_to_audio(pred_fix, encoder_model=codec, device=DECODE_DEVICE)\n",
    "\n",
    "    raw_rs = _resample_linear(audio_raw_b1[0], sr_raw, EVAL_SR)\n",
    "    fix_rs = _resample_linear(audio_fix_b1[0], sr_fix, EVAL_SR)\n",
    "\n",
    "    N = min(ref_rs.size, raw_rs.size, fix_rs.size)\n",
    "    ref_rs2 = ref_rs[:N]\n",
    "    raw_rs2 = raw_rs[:N]\n",
    "    fix_rs2 = fix_rs[:N]\n",
    "\n",
    "    print(f\"\\n{i+1}/{len(npz_paths)} {p.name}  pad_rate={pad_rate:.3f}  T={T}  C={pred.shape[0]}\")\n",
    "    print(\"Reference:\")\n",
    "    display(Audio(ref_rs2, rate=EVAL_SR))\n",
    "    print(\"Pred decode (RAW, may include PAD):\")\n",
    "    display(Audio(raw_rs2, rate=EVAL_SR))\n",
    "    print(\"Pred decode (PAD->0 before decode):\")\n",
    "    display(Audio(fix_rs2, rate=EVAL_SR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose “big model sounds like noise”: compare SMALL vs BIG on the same cache items.\n",
    "# Prints PAD rate + token diversity, and plays: reference, GT decode, small pred, big pred.\n",
    "\n",
    "from pathlib import Path\n",
    "import json, numpy as np, torch\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from midigroove_poc import expressivegrid as eg\n",
    "from midigroove_poc.eval import _load_audio_segment, _resample_linear\n",
    "from data.codecs import decode_tokens_to_audio\n",
    "\n",
    "CACHE_DIR = Path(\"cache/encodec_acoustic\")\n",
    "CKPT_SMALL = Path(\"artifacts/checkpoints/encodec_small_single_kit.pt\")\n",
    "CKPT_BIG   = Path(\"artifacts/checkpoints/encodec_big_single_kit.pt\")\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "DECODE_DEVICE = \"cuda:0\"\n",
    "EVAL_SR = 32000\n",
    "N_EX = 4\n",
    "\n",
    "# ---- pick a few test items deterministically ----\n",
    "manifest = sorted(CACHE_DIR.glob(\"manifest_midigroove_test_*.jsonl\"))[0]\n",
    "recs = [json.loads(x) for x in manifest.read_text().splitlines()[:N_EX]]\n",
    "npz_paths = [Path(r[\"npz\"]) for r in recs]\n",
    "print(\"NPZs:\", [p.name for p in npz_paths])\n",
    "\n",
    "def build_model(ckpt_path: Path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state = ckpt[\"model\"]\n",
    "    cfg = ckpt.get(\"cfg\", {}) if isinstance(ckpt.get(\"cfg\", {}), dict) else {}\n",
    "    num_codebooks = int(ckpt[\"num_codebooks\"])\n",
    "    in_dim = int(ckpt[\"in_dim\"])\n",
    "    codec = str(cfg.get(\"encoder_model\", \"encodec\") or \"encodec\").strip().lower()\n",
    "\n",
    "    cfg2 = dict(cfg)\n",
    "    if \"use_kit_name\" not in cfg2:\n",
    "        cfg2[\"use_kit_name\"] = bool(\"kit_name_emb.weight\" in state)\n",
    "\n",
    "    if \"vocab_size\" not in cfg2:\n",
    "        vs = eg._infer_vocab_size_from_state_dict(state, num_codebooks=num_codebooks)\n",
    "        if vs is not None:\n",
    "            cfg2[\"vocab_size\"] = int(vs)\n",
    "    if \"vocab_size\" in cfg2 and (\"pad_id\" not in cfg2 or \"codebook_size\" not in cfg2):\n",
    "        vs2 = int(cfg2.get(\"vocab_size\", 0) or 0)\n",
    "        if vs2 > 1:\n",
    "            cfg2.setdefault(\"pad_id\", vs2 - 1)\n",
    "            cfg2.setdefault(\"codebook_size\", vs2 - 1)\n",
    "\n",
    "    codebook_size = int(cfg2.get(\"codebook_size\", eg._default_codebook_size_for_encoder(codec)))\n",
    "    pad_id = int(cfg2.get(\"pad_id\", eg._pad_id_for_codebook(codebook_size)))\n",
    "    vocab_size = int(cfg2.get(\"vocab_size\", eg._vocab_size_for_codebook(codebook_size)))\n",
    "    cfg2[\"codebook_size\"] = codebook_size\n",
    "    cfg2[\"pad_id\"] = pad_id\n",
    "    cfg2[\"vocab_size\"] = vocab_size\n",
    "\n",
    "    model = eg._build_model(num_codebooks=num_codebooks, in_dim=in_dim, cfg=cfg2)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.to(torch.device(DEVICE)).eval()\n",
    "    return model, cfg2, codec, pad_id\n",
    "\n",
    "def load_item(npz_path: Path):\n",
    "    with np.load(npz_path, allow_pickle=False) as d:\n",
    "        ex = {k: np.asarray(d[k]) for k in d.files}\n",
    "\n",
    "    drum_hit = ex[\"drum_hit\"].astype(np.float32)\n",
    "    drum_vel = ex.get(\"drum_vel\", np.zeros_like(drum_hit)).astype(np.float32)\n",
    "    drum_sus = ex.get(\"drum_sustain\", np.zeros_like(drum_hit)).astype(np.float32)\n",
    "    hh_cc4   = ex.get(\"hh_open_cc4\", np.zeros((drum_hit.shape[1],), np.float32)).astype(np.float32)\n",
    "\n",
    "    beat_pos = ex[\"beat_pos\"].astype(np.int64)\n",
    "    bpm = float(ex.get(\"bpm\", 120.0))\n",
    "    drummer_id = int(ex.get(\"drummer_id\", 0))\n",
    "    kit_name_id = int(ex.get(\"kit_name_id\", 0))\n",
    "    tgt = ex[\"tgt\"].astype(np.int64)\n",
    "\n",
    "    audio_path = Path(str(ex[\"audio_path\"].item()))\n",
    "    sr_native = int(ex[\"sr\"].item())\n",
    "    start_sec = float(ex[\"start_sec\"].item())\n",
    "    window_seconds = float(ex[\"window_seconds\"].item())\n",
    "    start_sample = int(round(start_sec * sr_native))\n",
    "    window_samples = int(round(window_seconds * sr_native))\n",
    "    ref, sr_ref = _load_audio_segment(audio_path, start_sample=start_sample, num_samples=window_samples)\n",
    "    ref_rs = _resample_linear(ref, sr_ref, EVAL_SR)\n",
    "\n",
    "    return dict(\n",
    "        drum_hit=drum_hit, drum_vel=drum_vel, drum_sus=drum_sus, hh_cc4=hh_cc4,\n",
    "        beat_pos=beat_pos, bpm=bpm, drummer_id=drummer_id, kit_name_id=kit_name_id,\n",
    "        tgt=tgt, ref_rs=ref_rs\n",
    "    )\n",
    "\n",
    "def predict_tokens(model, cfg, pad_id, ex):\n",
    "    include_sustain = bool(cfg.get(\"include_sustain\", False))\n",
    "    include_hh_cc4  = bool(cfg.get(\"include_hh_cc4\", False))\n",
    "\n",
    "    pieces = [ex[\"drum_hit\"], ex[\"drum_vel\"]]\n",
    "    if include_sustain:\n",
    "        pieces.append(ex[\"drum_sus\"])\n",
    "    if include_hh_cc4:\n",
    "        pieces.append(ex[\"hh_cc4\"][None, :])\n",
    "    grid = np.concatenate(pieces, axis=0).astype(np.float32)\n",
    "    T = grid.shape[1]\n",
    "\n",
    "    grid_t = torch.from_numpy(grid).unsqueeze(0).to(DEVICE)\n",
    "    beat_t = torch.from_numpy(ex[\"beat_pos\"]).unsqueeze(0).to(DEVICE)\n",
    "    bpm_t  = torch.tensor([ex[\"bpm\"]], dtype=torch.float32, device=DEVICE)\n",
    "    dr_t   = torch.tensor([ex[\"drummer_id\"]], dtype=torch.long, device=DEVICE)\n",
    "    kit_t  = torch.tensor([ex[\"kit_name_id\"]], dtype=torch.long, device=DEVICE)\n",
    "    valid  = torch.ones((1, T), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logits = model(\n",
    "            grid=grid_t, beat_pos=beat_t, bpm=bpm_t, drummer_id=dr_t,\n",
    "            kit_name_id=kit_t if bool(cfg.get(\"use_kit_name\", True)) else None,\n",
    "            valid_mask=valid\n",
    "        )\n",
    "        pred = logits.argmax(dim=-1).squeeze(0).to(torch.long).cpu()  # [C,T]\n",
    "\n",
    "    pad_rate = float((pred == int(pad_id)).float().mean().item())\n",
    "    uniq = int(torch.unique(pred).numel())\n",
    "    total = int(pred.numel())\n",
    "    return pred, pad_rate, uniq, total\n",
    "\n",
    "model_s, cfg_s, codec_s, pad_s = build_model(CKPT_SMALL)\n",
    "model_b, cfg_b, codec_b, pad_b = build_model(CKPT_BIG)\n",
    "\n",
    "assert codec_s == codec_b, (codec_s, codec_b)\n",
    "codec = codec_s\n",
    "\n",
    "print(\"codec:\", codec)\n",
    "print(\"small pad_id:\", pad_s, \"big pad_id:\", pad_b)\n",
    "\n",
    "for i, npz in enumerate(npz_paths, 1):\n",
    "    ex = load_item(npz)\n",
    "\n",
    "    pred_s, pad_rate_s, uniq_s, total_s = predict_tokens(model_s, cfg_s, pad_s, ex)\n",
    "    pred_b, pad_rate_b, uniq_b, total_b = predict_tokens(model_b, cfg_b, pad_b, ex)\n",
    "\n",
    "    # decode (also decode GT)\n",
    "    tgt = torch.from_numpy(ex[\"tgt\"]).to(torch.long)\n",
    "    tgt_clean = torch.where(tgt == pad_s, torch.zeros_like(tgt), tgt)  # safe even if no PAD\n",
    "    pred_s_clean = torch.where(pred_s == pad_s, torch.zeros_like(pred_s), pred_s)\n",
    "    pred_b_clean = torch.where(pred_b == pad_b, torch.zeros_like(pred_b), pred_b)\n",
    "\n",
    "    gt_b1, sr_gt = decode_tokens_to_audio(tgt_clean, encoder_model=codec, device=DECODE_DEVICE)\n",
    "    s_b1,  sr_s  = decode_tokens_to_audio(pred_s_clean, encoder_model=codec, device=DECODE_DEVICE)\n",
    "    b_b1,  sr_b  = decode_tokens_to_audio(pred_b_clean, encoder_model=codec, device=DECODE_DEVICE)\n",
    "\n",
    "    ref = ex[\"ref_rs\"]\n",
    "    gt  = _resample_linear(gt_b1[0], sr_gt, EVAL_SR)\n",
    "    s   = _resample_linear(s_b1[0],  sr_s,  EVAL_SR)\n",
    "    b   = _resample_linear(b_b1[0],  sr_b,  EVAL_SR)\n",
    "    N = min(ref.size, gt.size, s.size, b.size)\n",
    "    ref, gt, s, b = ref[:N], gt[:N], s[:N], b[:N]\n",
    "\n",
    "    print(f\"\\n[{i}/{len(npz_paths)}] {npz.name}\")\n",
    "    print(f\"SMALL: pad_rate={pad_rate_s:.4f}, unique_tokens={uniq_s}/{total_s}\")\n",
    "    print(f\"BIG:   pad_rate={pad_rate_b:.4f}, unique_tokens={uniq_b}/{total_b}\")\n",
    "\n",
    "    print(\"Reference (original segment):\"); display(Audio(ref, rate=EVAL_SR))\n",
    "    print(\"GT decode (oracle codec reconstruction):\"); display(Audio(gt, rate=EVAL_SR))\n",
    "    print(\"SMALL pred decode:\"); display(Audio(s, rate=EVAL_SR))\n",
    "    print(\"BIG pred decode:\"); display(Audio(b, rate=EVAL_SR))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ksoil_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
